{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"flowerpower-io Documentation","text":"<p>Welcome to the documentation for <code>flowerpower-io</code>, a powerful Python library designed for efficient data processing and integration.</p> <p><code>flowerpower-io</code> streamlines common data workflows, providing robust tools for loading, transforming, and saving data with ease. Its key features include: *   Flexible Data Loading: Support for various data sources including Parquet, JSON, Delta Tables, and more. *   Efficient Data Saving: Seamlessly save data to different formats and systems. *   Integration with NATS and JetStream: Leverage real-time data streaming capabilities. *   Extensible Architecture: Easily integrate with existing data pipelines and tools.</p> <p>Get Started</p> <p>GitHub Repository </p>"},{"location":"advanced/","title":"Advanced Usage","text":"<p>This section delves into more advanced features and configurations of <code>flowerpower-io</code>, helping you optimize your data pipelines and integrate with complex systems.</p>"},{"location":"advanced/#custom-loaders-and-savers","title":"Custom Loaders and Savers","text":"<p><code>flowerpower-io</code> is designed to be extensible. You can create custom loaders and savers to handle data formats or systems not natively supported.</p> <p>To create a custom loader, inherit from <code>flowerpower_io.base.BaseFileReader</code> or <code>flowerpower_io.base.BaseDatasetReader</code> and implement the necessary methods. Similarly, for a custom saver, inherit from <code>flowerpower_io.base.BaseFileWriter</code> or <code>flowerpower_io.base.BaseDatasetWriter</code> and implement the <code>write</code> method.</p> <pre><code>from flowerpower_io.base import BaseFileReader, BaseFileWriter\nimport pandas as pd\n\nclass CustomTextLoader(BaseFileReader):\n    def __init__(self, filepath):\n        super().__init__(path=filepath, format=\"txt\")\n\n    def load(self) -&gt; pd.DataFrame:\n        with open(self.filepath, 'r') as f:\n            content = f.readlines()\n        # Assume each line is a record, parse as needed\n        data = [{\"line_num\": i, \"content\": line.strip()} for i, line in enumerate(content)]\n        return pd.DataFrame(data)\n\nclass CustomTextSaver(BaseFileWriter):\n    def __init__(self, filepath):\n        super().__init__(path=filepath, format=\"txt\")\n\n    def write(self, df: pd.DataFrame):\n        with open(self.filepath, 'w') as f:\n            for index, row in df.iterrows():\n                f.write(f\"{row['line_num']}: {row['content']}\\n\")\n\n# Example usage (conceptual)\n# loader = CustomTextLoader(\"my_data.txt\")\n# df = loader.to_polars()  # or .to_pandas(), .to_pyarrow_table()\n# saver = CustomTextSaver(\"output.txt\")\n# saver.write(df)\n</code></pre>"},{"location":"advanced/#performance-optimization","title":"Performance Optimization","text":"<p>For large datasets, consider the following:</p> <ul> <li>Batch Processing: When working with streaming data (e.g., MQTT), use the <code>batch_size</code> parameter to process data in batches rather than individually to reduce overhead.</li> <li>Efficient Data Formats: Utilize binary and columnar formats like Parquet or Delta Lake for faster reads and writes compared to text-based formats like CSV or JSON.</li> <li>Leverage Polars/PyArrow: <code>flowerpower-io</code> internally uses Polars DataFrames and PyArrow tables where appropriate for high-performance data manipulation. Use the <code>to_polars()</code> method with <code>lazy=True</code> for better performance with large datasets.</li> <li>Threading: Enable multi-threaded processing with <code>use_threads=True</code> for faster file operations.</li> <li>Data Type Optimization: Use <code>opt_dtypes=True</code> to automatically optimize data types for memory efficiency.</li> </ul>"},{"location":"advanced/#integration-with-external-systems","title":"Integration with External Systems","text":"<p><code>flowerpower-io</code>'s modular design facilitates integration with various external systems.</p>"},{"location":"advanced/#message-queues-nats-kafka","title":"Message Queues (NATS, Kafka)","text":"<p>Beyond direct loader/saver usage, <code>flowerpower-io</code> can act as a component within a larger message-driven architecture. For example, a custom loader could consume from a Kafka topic, and a saver could publish to another NATS subject after transformation.</p> <p>The library includes an <code>MQTTReader</code> class for handling MQTT payloads, which can be extended for other message queue systems:</p> <pre><code>from flowerpower_io.loader.mqtt import PayloadReader\n\n# Handle incoming MQTT messages\npayload_reader = PayloadReader(payload=message_payload, topic=\"sensor/data\")\ndf = payload_reader.to_polars()\n</code></pre>"},{"location":"advanced/#data-warehouses","title":"Data Warehouses","text":"<p>Directly load data into or save data from data warehouses like Snowflake, BigQuery, or Redshift using the appropriate database loaders/savers (e.g., <code>PostgreSQLReader</code>, <code>MySQLWriter</code>) and configuring the connection details.</p>"},{"location":"advanced/#cloud-storage","title":"Cloud Storage","text":"<p><code>flowerpower-io</code> supports various cloud storage backends through the <code>BaseFileIO</code> class, including:</p> <ul> <li>AWS S3 (via <code>AwsStorageOptions</code>)</li> <li>Google Cloud Storage (via <code>GcsStorageOptions</code>)</li> <li>Azure Blob Storage (via <code>AzureStorageOptions</code>)</li> <li>GitHub and GitLab repositories</li> </ul>"},{"location":"advanced/#advanced-data-processing","title":"Advanced Data Processing","text":""},{"location":"advanced/#sql-integration","title":"SQL Integration","text":"<p>The library provides SQL-based filtering capabilities through the <code>filter()</code> method, supporting SQL expressions, Polars expressions, and PyArrow compute expressions:</p> <pre><code># Using SQL expressions\nloader = ParquetFileReader(\"data.parquet\")\nfiltered_df = loader.filter(\"column1 &gt; 100 AND column2 = 'active'\")\n\n# Using Polars expressions\nfiltered_df = loader.filter(pl.col(\"column1\") &gt; 100)\n</code></pre>"},{"location":"advanced/#duckdb-and-datafusion-integration","title":"DuckDB and DataFusion Integration","text":"<p>Register data directly in DuckDB or DataFusion for advanced SQL analytics:</p> <pre><code># Register in DuckDB\nloader = ParquetFileReader(\"large_dataset.parquet\")\nconn = loader.register_in_duckdb(name=\"my_table\")\nresult = conn.execute(\"SELECT COUNT(*) FROM my_table\").fetchone()\n\n# Register in DataFusion\nctx = loader.register_in_datafusion(name=\"my_data\")\n# Perform DataFusion operations\n</code></pre>"},{"location":"advanced/#error-handling-and-logging","title":"Error Handling and Logging","text":"<p>Implement robust error handling around <code>flowerpower-io</code> operations. The library provides clear exceptions for common issues. Integrate with your application's logging framework to monitor data pipeline health and troubleshoot issues effectively.</p> <pre><code>import logging\nfrom flowerpower_io.loader import ParquetFileReader\nfrom flowerpower_io.base import BaseFileReader\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\ntry:\n    loader = ParquetFileReader(\"non_existent_file.parquet\")\n    df = loader.to_polars()\n    logging.info(\"Data loaded successfully.\")\nexcept FileNotFoundError:\n    logging.error(\"File not found error during loading.\")\nexcept ValueError as e:\n    logging.error(f\"Value error: {e}\")\nexcept Exception as e:\n    logging.error(f\"An unexpected error occurred: {e}\")\n</code></pre>"},{"location":"advanced/#configuration-and-best-practices","title":"Configuration and Best Practices","text":""},{"location":"advanced/#connection-management","title":"Connection Management","text":"<p>For database operations, use connection pooling and proper connection management:</p> <pre><code># Reuse database connections for better performance\nfrom flowerpower_io.loader import PostgreSQLReader\n\n# Create connection once and reuse\ndb_reader = PostgreSQLReader(\n    connection_string=\"postgresql://user:pass@localhost:5432/mydb\",\n    table_name=\"my_table\"\n)\n\n# Multiple queries on the same connection\ndf1 = db_reader.to_polars(query=\"SELECT * FROM my_table WHERE created_at &gt; '2023-01-01'\")\ndf2 = db_reader.to_polars(query=\"SELECT COUNT(*) FROM my_table\")\n</code></pre>"},{"location":"advanced/#metadata-handling","title":"Metadata Handling","text":"<p>Leverage the built-in metadata tracking for data lineage and debugging:</p> <pre><code>loader = ParquetFileReader(\"data.parquet\")\ndf, metadata = loader.to_polars(metadata=True)\n\n# Access metadata\nprint(f\"Source path: {metadata['path']}\")\nprint(f\"Format: {metadata['format']}\")\nprint(f\"Number of files: {metadata['num_files']}\")\n</code></pre>"},{"location":"advanced/#memory-management","title":"Memory Management","text":"<p>For large datasets, use lazy evaluation and batch processing:</p> <p>```python</p>"},{"location":"advanced/#use-lazy-evaluation-for-memory-efficiency","title":"Use lazy evaluation for memory efficiency","text":"<p>loader = ParquetDatasetReader(\"large_dataset/\") lazy_df = loader.to_polars(lazy=True)</p>"},{"location":"advanced/#process-in-batches","title":"Process in batches","text":"<p>for batch in loader.iter_polars(batch_size=10000):     process_batch(batch)</p>"},{"location":"architecture/","title":"Architecture Overview","text":"<p>This section provides an overview of the <code>flowerpower-io</code> library's architecture, highlighting its core components and how they interact.</p>"},{"location":"architecture/#core-design-principles","title":"Core Design Principles","text":"<p><code>flowerpower-io</code> is designed with modularity, extensibility, and efficiency in mind. It separates concerns into distinct components for data loading, transformation, and saving, allowing for flexible and scalable data pipelines.</p>"},{"location":"architecture/#component-overview","title":"Component Overview","text":""},{"location":"architecture/#loaders","title":"Loaders","text":"<p>Loaders are responsible for ingesting data from various sources. Each loader is specialized for a particular data format or system.</p> <ul> <li>Supported Loaders: Parquet, JSON, Delta Tables, DuckDB, MQTT, MSSQL, MySQL, Oracle, PostgreSQL, PyDala, SQLite.</li> </ul>"},{"location":"architecture/#savers","title":"Savers","text":"<p>Savers are responsible for writing processed data to different destinations. Similar to loaders, each saver is tailored for specific output formats or systems.</p> <ul> <li>Supported Savers: CSV, Delta Tables, DuckDB, JSON, MQTT, MSSQL, MySQL, Oracle, Parquet, PostgreSQL, PyDala, SQLite.</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>This section provides various examples demonstrating the capabilities and usage of <code>flowerpower-io</code>. These examples cover common data loading and saving scenarios.</p>"},{"location":"examples/#example-1-loading-parquet-and-saving-to-json","title":"Example 1: Loading Parquet and Saving to JSON","text":"<p>This example shows how to load data from a Parquet file and save it as a JSON file.</p> <pre><code>from flowerpower_io.loader import ParquetLoader\nfrom flowerpower_io.saver import JsonSaver\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\n\n# Create a dummy Parquet file for demonstration\ntable = pa.table({'col1': [1, 2, 3], 'col2': ['A', 'B', 'C']})\npq.write_table(table, 'dummy.parquet')\n\n# Load data from Parquet\nparquet_loader = ParquetLoader('dummy.parquet')\ndf_parquet = parquet_loader.load()\nprint(\"Loaded DataFrame from Parquet:\")\nprint(df_parquet)\n\n# Save data to JSON\njson_saver = JsonSaver('output.json')\njson_saver.save(df_parquet)\nprint(\"\\nData saved to output.json\")\n\n# Clean up\nos.remove('dummy.parquet')\nos.remove('output.json')\n</code></pre>"},{"location":"examples/#example-2-loading-csv-and-saving-to-delta-table","title":"Example 2: Loading CSV and Saving to Delta Table","text":"<p>This example demonstrates loading data from a CSV file and saving it as a Delta Table.</p> <pre><code>from flowerpower_io.loader import CsvLoader\nfrom flowerpower_io.saver import DeltaTableSaver\nimport os\n\n# Create a dummy CSV file\ncsv_data = \"col1,col2\\n1,A\\n2,B\\n3,C\"\nwith open(\"dummy.csv\", \"w\") as f:\n    f.write(csv_data)\n\n# Load data from CSV\ncsv_loader = CsvLoader('dummy.csv')\ndf_csv = csv_loader.load()\nprint(\"Loaded DataFrame from CSV:\")\nprint(df_csv)\n\n# Save data to Delta Table\ndelta_saver = DeltaTableSaver('output_delta_table')\ndelta_saver.save(df_csv)\nprint(\"\\nData saved to output_delta_table (Delta Table format)\")\n\n# Clean up\nos.remove('dummy.csv')\n# For Delta Table, you might need to use delta-rs or similar to truly clean up\n# For simplicity, we just remove the directory\nimport shutil\nshutil.rmtree('output_delta_table')\n</code></pre>"},{"location":"examples/#example-3-using-mssql-loader-and-parquet-saver","title":"Example 3: Using MSSQL Loader and Parquet Saver","text":"<p>This example outlines how to connect to an MSSQL database and save the retrieved data to a Parquet file.</p> <pre><code>from flowerpower_io.loader import MssqlLoader\nfrom flowerpower_io.saver import ParquetSaver\n# This example requires a running MSSQL instance and appropriate credentials.\n# Replace placeholder values with your actual database connection details.\n\n# Example: Load data from MSSQL (conceptual)\n# mssql_loader = MssqlLoader(\n#     server='your_server',\n#     database='your_database',\n#     username='your_username',\n#     password='your_password',\n#     query='SELECT * FROM your_table'\n# )\n# df_mssql = mssql_loader.load()\n# print(\"Loaded DataFrame from MSSQL:\")\n# print(df_mssql)\n\n# Example: Save data to Parquet (conceptual)\n# parquet_saver = ParquetSaver('output_mssql.parquet')\n# parquet_saver.save(df_mssql)\n# print(\"\\nData saved to output_mssql.parquet\")\n</code></pre>"},{"location":"examples/#example-4-mqtt-loader-and-json-saver-real-time-data","title":"Example 4: MQTT Loader and JSON Saver (Real-time Data)","text":"<p>This example illustrates how <code>flowerpower-io</code> can interact with real-time data streams via MQTT, then save the incoming messages to a JSON file.</p> <pre><code>from flowerpower_io.loader import MqttLoader\nfrom flowerpower_io.saver import JsonSaver\nimport time\nimport json\nimport paho.mqtt.client as mqtt\nimport threading\nimport os\n\n# --- MQTT Broker Setup (Conceptual) ---\n# For a real scenario, you'd have an MQTT broker running (e.g., Mosquitto).\n# This example simulates messages being published to a topic.\n\nmqtt_topic = \"flowerpower/data\"\noutput_json_file = \"mqtt_output.json\"\n\ndef simulate_mqtt_publish():\n    client = mqtt.Client(\"publisher\")\n    client.connect(\"broker.hivemq.com\") # Using a public test broker\n    for i in range(3):\n        message = {\"timestamp\": time.time(), \"value\": i * 10}\n        client.publish(mqtt_topic, json.dumps(message))\n        print(f\"Published: {message}\")\n        time.sleep(1)\n    client.disconnect()\n\n# Start a thread to simulate MQTT publishing\npublisher_thread = threading.Thread(target=simulate_mqtt_publish)\npublisher_thread.start()\n\n# --- flowerpower-io MQTT Loader and JsonSaver ---\n# The MQTTLoader would typically run continuously to listen for messages.\n# For this example, we'll simulate a short listening period.\n\n# mqtt_loader = MqttLoader(\n#     broker_address=\"broker.hivemq.com\",\n#     topic=mqtt_topic,\n#     message_count=3 # Listen for 3 messages for this example\n# )\n#\n# json_saver = JsonSaver(output_json_file)\n#\n# print(f\"\\nListening for MQTT messages on topic: {mqtt_topic}...\")\n# # df_mqtt = mqtt_loader.load() # This would block until messages are received\n# # json_saver.save(df_mqtt)\n# print(f\"Messages saved to {output_json_file}\")\n\n# Clean up (if output file was created)\n# if os.path.exists(output_json_file):\n#     os.remove(output_json_file)\n</code></pre>"},{"location":"examples/#example-5-postgresql-database-operations","title":"Example 5: PostgreSQL Database Operations","text":"<p>This example demonstrates how to work with PostgreSQL databases using <code>flowerpower-io</code>.</p> <pre><code>from flowerpower_io.loader import PostgresLoader\nfrom flowerpower_io.saver import PostgresSaver\nimport pandas as pd\n\n# Create sample data\nsample_data = pd.DataFrame({\n    'id': [1, 2, 3],\n    'name': ['Alice', 'Bob', 'Charlie'],\n    'age': [25, 30, 35]\n})\n\n# This example requires a running PostgreSQL instance and appropriate credentials.\n# Replace placeholder values with your actual database connection details.\n\n# Example: Save data to PostgreSQL (conceptual)\n# postgres_saver = PostgresSaver(\n#     host='localhost',\n#     database='your_database',\n#     username='your_username',\n#     password='your_password',\n#     table_name='employees'\n# )\n# postgres_saver.save(sample_data)\n# print(\"Data saved to PostgreSQL database\")\n\n# Example: Load data from PostgreSQL (conceptual)\n# postgres_loader = PostgresLoader(\n#     host='localhost',\n#     database='your_database',\n#     username='your_username',\n#     password='your_password',\n#     query='SELECT * FROM employees'\n# )\n# df_postgres = postgres_loader.load()\n# print(\"Loaded DataFrame from PostgreSQL:\")\n# print(df_postgres)\n</code></pre>"},{"location":"examples/#example-6-multi-format-data-pipeline","title":"Example 6: Multi-format Data Pipeline","text":"<p>This example demonstrates a complete data pipeline that processes data through multiple formats.</p> <p>```python from flowerpower_io.loader import CsvLoader, ParquetLoader from flowerpower_io.saver import JsonSaver, ParquetSaver import pandas as pd import os</p>"},{"location":"examples/#create-sample-csv-data","title":"Create sample CSV data","text":"<p>csv_data = \"\"\"id,name,category,value 1,Product A,Electronics,100.50 2,Product B,Clothing,45.00 3,Product C,Electronics,299.99 4,Product D,Books,19.99 5,Product E,Clothing,89.50\"\"\"</p>"},{"location":"examples/#write-csv-file","title":"Write CSV file","text":"<p>with open(\"products.csv\", \"w\") as f:     f.write(csv_data)</p>"},{"location":"examples/#step-1-load-csv-data","title":"Step 1: Load CSV data","text":"<p>csv_loader = CsvLoader('products.csv') df_products = csv_loader.load() print(\"Step 1: Loaded CSV data\") print(df_products)</p>"},{"location":"examples/#step-2-save-as-parquet","title":"Step 2: Save as Parquet","text":"<p>parquet_saver = ParquetSaver('products.parquet') parquet_saver.save(df_products) print(\"\\nStep 2: Saved data as Parquet\")</p>"},{"location":"examples/#step-3-load-from-parquet","title":"Step 3: Load from Parquet","text":"<p>parquet_loader = ParquetLoader('products.parquet') df_from_parquet = parquet_loader.load() print(\"\\nStep 3: Loaded data from Parquet\") print(df_from_parquet)</p>"},{"location":"examples/#step-4-filter-and-save-as-json","title":"Step 4: Filter and save as JSON","text":"<p>electronics_products = df_from_parquet[df_from_parquet['category'] == 'Electronics'] json_saver = JsonSaver('electronics_products.json') json_saver.save(electronics_products) print(\"\\nStep 4: Filtered electronics products and saved as JSON\") print(electronics_products)</p>"},{"location":"examples/#clean-up","title":"Clean up","text":"<p>os.remove('products.csv') os.remove('products.parquet') os.remove('electronics_products.json')</p> <p>print(\"\\nData pipeline completed successfully!\")</p>"},{"location":"installation/","title":"Installation","text":"<p>To install <code>flowerpower-io</code>, ensure you have Python 3.8+ installed. We recommend using <code>uv</code> or <code>pixi</code> for dependency management.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li><code>pip</code> (Python package installer)</li> </ul>"},{"location":"installation/#recommended-installation-with-uv","title":"Recommended Installation with <code>uv</code>","text":"<p><code>uv</code> is a fast Python package installer and resolver.</p> <ol> <li> <p>Install <code>uv</code> (if you don't have it): <pre><code>pip install uv\n</code></pre></p> </li> <li> <p>Install <code>flowerpower-io</code>:     Navigate to your project directory and run:     <pre><code>uv pip install flowerpower-io\n</code></pre></p> </li> </ol>"},{"location":"installation/#recommended-installation-with-pixi","title":"Recommended Installation with <code>pixi</code>","text":"<p><code>pixi</code> is a modern cross-platform package manager.</p> <ol> <li> <p>Install <code>pixi</code> (if you don't have it):     Follow the instructions on the pixi website.</p> </li> <li> <p>Install <code>flowerpower-io</code>:     Navigate to your project directory and run:     <pre><code>pixi add flowerpower-io\n</code></pre></p> </li> </ol>"},{"location":"installation/#standard-installation-with-pip","title":"Standard Installation with <code>pip</code>","text":"<p>If you prefer using <code>pip</code> directly:</p> <pre><code>pip install flowerpower-io\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Permissions Error: If you encounter a permissions error, try installing with the <code>--user</code> flag:     <pre><code>pip install --user flowerpower-io\n</code></pre></li> <li>Virtual Environments: It's highly recommended to use a virtual environment to avoid conflicts with system-wide Python packages.     <code>``bash     python -m venv venv     source venv/bin/activate # On Windows, use</code>venv\\Scripts\\activate`     pip install flowerpower-io</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with <code>flowerpower-io</code> quickly. We'll walk through a simple example of loading data from a source and saving it.</p>"},{"location":"quickstart/#1-import-necessary-modules","title":"1. Import necessary modules","text":"<p>First, import the required components from <code>flowerpower-io</code>.</p> <pre><code>from flowerpower_io.loader import JsonLoader\nfrom flowerpower_io.saver import CsvSaver\n</code></pre>"},{"location":"quickstart/#2-prepare-your-data","title":"2. Prepare your data","text":"<p>For this example, let's assume you have a simple JSON file named <code>data.json</code>:</p> <pre><code>[\n  {\"id\": 1, \"name\": \"Alice\", \"age\": 30},\n  {\"id\": 2, \"name\": \"Bob\", \"age\": 24},\n  {\"id\": 3, \"name\": \"Charlie\", \"age\": 35}\n]\n</code></pre>"},{"location":"quickstart/#3-load-the-data","title":"3. Load the data","text":"<p>Use the <code>JsonLoader</code> to load the data from <code>data.json</code>.</p> <pre><code>loader = JsonLoader(\"data.json\")\ndf = loader.load()\nprint(df)\n</code></pre>"},{"location":"quickstart/#4-save-the-data","title":"4. Save the data","text":"<p>Now, let's save the loaded data to a CSV file named <code>output.csv</code>.</p> <pre><code>saver = CsvSaver(\"output.csv\")\nsaver.save(df)\nprint(\"Data successfully saved to output.csv\")\n</code></pre>"},{"location":"quickstart/#full-example","title":"Full Example","text":"<p>Here's the complete code for this quickstart example:</p> <pre><code>from flowerpower_io.loader import JsonLoader\nfrom flowerpower_io.saver import CsvSaver\n\n# Create a dummy JSON file for demonstration\njson_data = \"\"\"\n[\n  {\"id\": 1, \"name\": \"Alice\", \"age\": 30},\n  {\"id\": 2, \"name\": \"Bob\", \"age\": 24},\n  {\"id\": 3, \"name\": \"Charlie\", \"age\": 35}\n]\n\"\"\"\nwith open(\"data.json\", \"w\") as f:\n    f.write(json_data)\n\n# Load the data\nloader = JsonLoader(\"data.json\")\ndf = loader.load()\nprint(\"Loaded DataFrame:\")\nprint(df)\n\n# Save the data\nsaver = CsvSaver(\"output.csv\")\nsaver.save(df)\nprint(\"\\nData successfully saved to output.csv\")\n\n# Clean up dummy file\nimport os\nos.remove(\"data.json\")\nos.remove(\"output.csv\")\n</code></pre> <p>This simple example demonstrates the basic flow of loading and saving data using <code>flowerpower-io</code>. Explore other loaders and savers for more advanced use cases!</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the <code>flowerpower-io</code> API reference documentation. This section provides detailed information about all public classes, functions, and methods available in the library.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>The <code>flowerpower-io</code> library provides a unified interface for reading and writing data from various sources and formats. The API is organized into several modules:</p> <ul> <li>Base Classes - Core classes for file and database operations</li> <li>Metadata Functions - Functions for extracting metadata from data sources</li> <li>Loader Classes - Classes for reading data from various sources</li> <li>Saver Classes - Classes for writing data to various destinations</li> </ul>"},{"location":"api/#quick-navigation","title":"Quick Navigation","text":""},{"location":"api/#base-classes","title":"Base Classes","text":"<p>The base classes form the foundation of the library and provide common functionality for all I/O operations.</p> <ul> <li>BaseFileIO - Base class for file I/O operations</li> <li>BaseFileReader - Base class for file reading operations</li> <li>BaseDatasetReader - Base class for dataset reading operations</li> <li>BaseFileWriter - Base class for file writing operations</li> <li>BaseDatasetWriter - Base class for dataset writing operations</li> <li>BaseDatabaseIO - Base class for database operations</li> <li>BaseDatabaseReader - Base class for database reading operations</li> <li>BaseDatabaseWriter - Base class for database writing operations</li> </ul>"},{"location":"api/#metadata-functions","title":"Metadata Functions","text":"<p>Metadata functions help you understand the structure and properties of your data before processing it.</p> <ul> <li>get_dataframe_metadata - Extract metadata from DataFrames</li> <li>get_pyarrow_table_metadata - Extract metadata from PyArrow Tables</li> <li>get_pyarrow_dataset_metadata - Extract metadata from PyArrow Datasets</li> <li>get_duckdb_relation_metadata - Extract metadata from DuckDB relations</li> <li>get_datafusion_relation_metadata - Extract metadata from DataFusion relations</li> <li>get_file_metadata - Extract metadata from files</li> <li>get_database_metadata - Extract metadata from database tables</li> <li>get_metadata - Generic metadata extraction function</li> </ul>"},{"location":"api/#loader-classes","title":"Loader Classes","text":"<p>Loader classes provide specialized functionality for reading data from various sources.</p>"},{"location":"api/#file-loaders","title":"File Loaders","text":"<ul> <li>CSVLoader - Load data from CSV files</li> <li>ParquetLoader - Load data from Parquet files</li> <li>JSONLoader - Load data from JSON files</li> <li>DeltaTableLoader - Load data from Delta Lake tables</li> <li>PydalaLoader - Load data from Pydala datasets</li> <li>MQTTLoader - Load data from MQTT messages</li> </ul>"},{"location":"api/#database-loaders","title":"Database Loaders","text":"<ul> <li>SQLiteLoader - Load data from SQLite databases</li> <li>DuckDBLoader - Load data from DuckDB databases</li> <li>PostgreSQLLoader - Load data from PostgreSQL databases</li> <li>MySQLLoader - Load data from MySQL databases</li> <li>MSSQLLoader - Load data from Microsoft SQL Server databases</li> <li>OracleLoader - Load data from Oracle databases</li> </ul>"},{"location":"api/#saver-classes","title":"Saver Classes","text":"<p>Saver classes provide specialized functionality for writing data to various destinations.</p>"},{"location":"api/#file-savers","title":"File Savers","text":"<ul> <li>CSVSaver - Save data to CSV files</li> <li>ParquetSaver - Save data to Parquet files</li> <li>JSONSaver - Save data to JSON files</li> <li>DeltaTableSaver - Save data to Delta Lake tables</li> <li>PydalaSaver - Save data to Pydala datasets</li> <li>MQTTSaver - Save data to MQTT messages</li> </ul>"},{"location":"api/#database-savers","title":"Database Savers","text":"<ul> <li>SQLiteSaver - Save data to SQLite databases</li> <li>DuckDBSaver - Save data to DuckDB databases</li> <li>PostgreSQLSaver - Save data to PostgreSQL databases</li> <li>MySQLSaver - Save data to MySQL databases</li> <li>MSSQLSaver - Save data to Microsoft SQL Server databases</li> <li>OracleSaver - Save data to Oracle databases</li> </ul>"},{"location":"api/#usage-examples","title":"Usage Examples","text":""},{"location":"api/#basic-file-operations","title":"Basic File Operations","text":"<pre><code>from flowerpower_io import CSVLoader, ParquetSaver\n\n# Load data from CSV\nloader = CSVLoader(\"data.csv\")\ndf = loader.to_polars()\n\n# Save data to Parquet\nsaver = ParquetSaver(\"output/\")\nsaver.write(df)\n</code></pre>"},{"location":"api/#database-operations","title":"Database Operations","text":"<pre><code>from flowerpower_io import PostgreSQLLoader, SQLiteSaver\n\n# Load from PostgreSQL\nloader = PostgreSQLLoader(\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\",\n    table_name=\"users\"\n)\ndf = loader.to_polars()\n\n# Save to SQLite\nsaver = SQLiteSaver(\n    path=\"database.db\",\n    table_name=\"users\"\n)\nsaver.write(df)\n</code></pre>"},{"location":"api/#metadata-extraction","title":"Metadata Extraction","text":"<pre><code>from flowerpower_io.metadata import get_dataframe_metadata\n\n# Get metadata from DataFrame\nmetadata = get_dataframe_metadata(df)\nprint(metadata)\n</code></pre>"},{"location":"api/#common-patterns","title":"Common Patterns","text":""},{"location":"api/#reading-multiple-files","title":"Reading Multiple Files","text":"<pre><code>from flowerpower_io import ParquetLoader\n\n# Load multiple Parquet files\nloader = ParquetLoader(\"data/*.parquet\")\ndf = loader.to_polars()\n</code></pre>"},{"location":"api/#writing-with-partitioning","title":"Writing with Partitioning","text":"<pre><code>from flowerpower_io import ParquetSaver\n\n# Save with partitioning\nsaver = ParquetSaver(\n    path=\"output/\",\n    partition_by=\"category\",\n    compression=\"zstd\"\n)\nsaver.write(df)\n</code></pre>"},{"location":"api/#database-connection-management","title":"Database Connection Management","text":"<pre><code>from flowerpower_io import PostgreSQLLoader\n\n# Using context manager for connection\nwith PostgreSQLLoader(\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n) as loader:\n    df = loader.to_polars()\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>The library provides comprehensive error handling for various scenarios:</p> <pre><code>from flowerpower_io import CSVLoader\n\ntry:\n    loader = CSVLoader(\"nonexistent.csv\")\n    df = loader.to_polars()\nexcept FileNotFoundError:\n    print(\"File not found\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"api/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use <code>opt_dtypes=True</code> for better memory efficiency</li> <li>Use <code>batch_size</code> for large datasets</li> <li>Use <code>concat=False</code> when working with multiple files separately</li> <li>Use appropriate compression for your data format</li> <li>Use partitioning for large datasets</li> </ol>"},{"location":"api/#see-also","title":"See Also","text":"<ul> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>Advanced Usage</li> <li>Architecture Overview</li> </ul>"},{"location":"api/base/","title":"Base Classes","text":"<p>The <code>flowerpower-io</code> library provides several base classes that form the foundation for all I/O operations. These classes handle common functionality for file and database operations, including data conversion, filtering, and metadata management.</p>"},{"location":"api/base/#basefileio","title":"BaseFileIO","text":"<pre><code>class BaseFileIO(msgspec.Struct, gc=False)\n</code></pre> <p>The base class for file I/O operations supporting various storage backends.</p>"},{"location":"api/base/#parameters","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to file(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot)."},{"location":"api/base/#properties","title":"Properties","text":"Property Type Description <code>protocol</code> <code>str</code> Get the protocol of the filesystem. <code>_base_path</code> <code>str</code> Get the base path for the filesystem. <code>_path</code> <code>str \\| list[str]</code> Get the processed path. <code>_glob_path</code> <code>str \\| list[str]</code> Get the glob path for file operations. <code>_root_path</code> <code>str</code> Get the root path for the filesystem."},{"location":"api/base/#methods","title":"Methods","text":"Method Signature Description <code>list_files</code> <code>() -&gt; list[str]</code> List files matching the path pattern."},{"location":"api/base/#example","title":"Example","text":"<pre><code>from flowerpower_io import BaseFileIO\n\n# Initialize with local path\nfile_io = BaseFileIO(path=\"data/\")\n\n# Initialize with S3 path\nfile_io = BaseFileIO(\n    path=\"s3://bucket/path/to/files\",\n    storage_options=AwsStorageOptions(\n        key=\"access_key\",\n        secret=\"secret_key\"\n    )\n)\n\n# List files\nfiles = file_io.list_files()\n</code></pre>"},{"location":"api/base/#basefilereader","title":"BaseFileReader","text":"<pre><code>class BaseFileReader(BaseFileIO, gc=False)\n</code></pre> <p>The base class for file loading operations supporting various file formats.</p>"},{"location":"api/base/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>include_file_path</code> <code>bool</code> Include file path in the output DataFrame. Default: <code>False</code> <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>True</code> <code>batch_size</code> <code>int \\| None</code> Batch size for reading data. Default: <code>None</code> <code>opt_dtypes</code> <code>bool</code> Optimize data types. Default: <code>False</code> <code>use_threads</code> <code>bool</code> Use threads for reading data. Default: <code>True</code> <code>conn</code> <code>duckdb.DuckDBPyConnection \\| None</code> DuckDB connection instance. <code>ctx</code> <code>datafusion.SessionContext \\| None</code> DataFusion session context instance. <code>jsonlines</code> <code>bool \\| None</code> Whether the file is in JSON Lines format. <code>partitioning</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>verbose</code> <code>bool \\| None</code> Verbose output."},{"location":"api/base/#methods_1","title":"Methods","text":"Method Signature Description <code>to_pandas</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame \\| list[pd.DataFrame], dict[str, Any]]</code> Convert data to Pandas DataFrame(s). <code>iter_pandas</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pd.DataFrame, None, None]</code> Iterate over Pandas DataFrames. <code>to_polars</code> <code>(lazy: bool = False, metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| tuple[pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame], dict[str, Any]]</code> Convert data to Polars DataFrame or LazyFrame. <code>iter_polars</code> <code>(lazy: bool = False, reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pl.DataFrame \\| pl.LazyFrame, None, None]</code> Iterate over Polars DataFrames or LazyFrames. <code>to_pyarrow_table</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pa.Table \\| list[pa.Table] \\| tuple[pa.Table \\| list[pa.Table], dict[str, Any]]</code> Convert data to PyArrow Table(s). <code>iter_pyarrow_table</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pa.Table, None, None]</code> Iterate over PyArrow Tables. <code>to_duckdb_relation</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(conn: duckdb.DuckDBPyConnection, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Register data in DuckDB. <code>to_duckdb</code> <code>(as_relation: bool = True, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]] \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Convert data to DuckDB relation or register in DuckDB. <code>register_in_datafusion</code> <code>(ctx: datafusion.SessionContext, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; datafusion.SessionContext \\| tuple[datafusion.SessionContext, dict[str, Any]]</code> Register data in DataFusion. <code>filter</code> <code>(filter_expr: str \\| pl.Expr \\| pa.compute.Expression) -&gt; pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| list[pa.Table]</code> Filter data based on a filter expression. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/base/#example_1","title":"Example","text":"<pre><code>from flowerpower_io import BaseFileReader\n\n# Initialize file reader\nreader = BaseFileReader(\n    path=\"data/*.csv\",\n    format=\"csv\",\n    include_file_path=True,\n    concat=True\n)\n\n# Convert to different formats\ndf_pandas = reader.to_pandas()\ndf_polars = reader.to_polars()\ntable_arrow = reader.to_pyarrow_table()\n\n# Register in DuckDB\nconn = reader.register_in_duckdb(name=\"my_table\")\n\n# Filter data\nfiltered = reader.filter(\"column &gt; 100\")\n</code></pre>"},{"location":"api/base/#basedatasetreader","title":"BaseDatasetReader","text":"<pre><code>class BaseDatasetReader(BaseFileReader, gc=False)\n</code></pre> <p>The base class for dataset loading operations supporting various file formats.</p>"},{"location":"api/base/#parameters_2","title":"Parameters","text":"Parameter Type Description <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>_dataset</code> <code>pds.Dataset \\| None</code> Internal PyArrow dataset. <code>_pydala_dataset</code> <code>Any \\| None</code> Internal Pydala dataset."},{"location":"api/base/#methods_2","title":"Methods","text":"Method Signature Description <code>to_pyarrow_dataset</code> <code>(metadata: bool = False, reload: bool = False, **kwargs) -&gt; pds.Dataset \\| tuple[pds.Dataset, dict[str, Any]]</code> Convert data to PyArrow Dataset. <code>to_pydala_dataset</code> <code>(metadata: bool = False, reload: bool = False, **kwargs) -&gt; ParquetDataset \\| tuple[ParquetDataset, dict[str, Any]]</code> Convert data to Pydala ParquetDataset."},{"location":"api/base/#example_2","title":"Example","text":"<pre><code>from flowerpower_io import BaseDatasetReader\n\n# Initialize dataset reader\nreader = BaseDatasetReader(\n    path=\"dataset/\",\n    format=\"parquet\",\n    schema_=pa.schema([\n        pa.field(\"id\", pa.int64()),\n        pa.field(\"name\", pa.string())\n    ])\n)\n\n# Convert to dataset\ndataset = reader.to_pyarrow_dataset()\n</code></pre>"},{"location":"api/base/#basefilewriter","title":"BaseFileWriter","text":"<pre><code>class BaseFileWriter(BaseFileIO, gc=False)\n</code></pre> <p>The base class for file writing operations supporting various storage backends.</p>"},{"location":"api/base/#parameters_3","title":"Parameters","text":"Parameter Type Description <code>basename</code> <code>str \\| None</code> Basename for the output file(s). <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>mode</code> <code>str</code> Write mode (append, overwrite, delete_matching, error_if_exists). Default: \"append\" <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code>"},{"location":"api/base/#methods_3","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pd.DataFrame \\| dict[str, Any]]], basename: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None, mode: str \\| None = None, **kwargs) -&gt; dict[str, Any]</code> Write data to file. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/base/#example_3","title":"Example","text":"<pre><code>from flowerpower_io import BaseFileWriter\n\n# Initialize file writer\nwriter = BaseFileWriter(\n    path=\"output/\",\n    format=\"parquet\",\n    basename=\"result\",\n    mode=\"append\"\n)\n\n# Write data\nmetadata = writer.write(df)\n</code></pre>"},{"location":"api/base/#basedatasetwriter","title":"BaseDatasetWriter","text":"<pre><code>class BaseDatasetWriter(BaseFileWriter, gc=False)\n</code></pre> <p>The base class for dataset writing operations supporting various file formats.</p>"},{"location":"api/base/#parameters_4","title":"Parameters","text":"Parameter Type Description <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>partition_by</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>partitioning_flavor</code> <code>str \\| None</code> Partitioning flavor for the dataset. <code>compression</code> <code>str</code> Compression codec for the dataset. Default: \"zstd\" <code>row_group_size</code> <code>int \\| None</code> Row group size for the dataset. Default: 250000 <code>max_rows_per_file</code> <code>int \\| None</code> Maximum number of rows per file. Default: 2500000 <code>is_pydala_dataset</code> <code>bool</code> Write data as a Pydala ParquetDataset. Default: <code>False</code>"},{"location":"api/base/#methods_4","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None, mode: str \\| None = None, delta_subset: str \\| None = None, alter_schema: bool = False, update_metadata: bool = True, timestamp_column: str \\| None = None, verbose: bool = False, **kwargs) -&gt; dict[str, Any]</code> Write data to dataset."},{"location":"api/base/#example_4","title":"Example","text":"<pre><code>from flowerpower_io import BaseDatasetWriter\n\n# Initialize dataset writer\nwriter = BaseDatasetWriter(\n    path=\"output_dataset/\",\n    format=\"parquet\",\n    partition_by=\"category\",\n    compression=\"zstd\"\n)\n\n# Write data with partitioning\nmetadata = writer.write(df)\n</code></pre>"},{"location":"api/base/#basedatabaseio","title":"BaseDatabaseIO","text":"<pre><code>class BaseDatabaseIO(msgspec.Struct, gc=False)\n</code></pre> <p>The base class for database read/write operations supporting various database systems.</p>"},{"location":"api/base/#parameters_5","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type (sqlite, duckdb, postgres, mysql, mssql, oracle). <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite or DuckDB databases. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context."},{"location":"api/base/#methods_5","title":"Methods","text":"Method Signature Description <code>execute</code> <code>(query: str, cursor: bool = True, **query_kwargs)</code> Execute a SQL query. <code>create_engine</code> <code>() -&gt; sqlalchemy.engine.Engine</code> Create a SQLAlchemy engine. <code>connect</code> <code>() -&gt; sqlite3.Connection \\| duckdb.DuckDBPyConnection \\| sqlalchemy.engine.Connection</code> Create a database connection."},{"location":"api/base/#example_5","title":"Example","text":"<pre><code>from flowerpower_io import BaseDatabaseIO\n\n# Initialize database IO\ndb_io = BaseDatabaseIO(\n    type_=\"sqlite\",\n    table_name=\"my_table\",\n    path=\"database.db\"\n)\n\n# Execute query\nresult = db_io.execute(\"SELECT * FROM my_table\")\n</code></pre>"},{"location":"api/base/#basedatabasereader","title":"BaseDatabaseReader","text":"<pre><code>class BaseDatabaseReader(BaseDatabaseIO, gc=False)\n</code></pre> <p>The base class for database read operations supporting various database systems.</p>"},{"location":"api/base/#parameters_6","title":"Parameters","text":"Parameter Type Description <code>query</code> <code>str \\| None</code> SQL query to execute."},{"location":"api/base/#methods_6","title":"Methods","text":"Method Signature Description <code>to_polars</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pl.DataFrame \\| tuple[pl.DataFrame, dict[str, Any]]</code> Convert data to Polars DataFrame. <code>to_pandas</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame, dict[str, Any]]</code> Convert data to Pandas DataFrame. <code>to_pyarrow_table</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pa.Table \\| tuple[pa.Table, dict[str, Any]]</code> Convert data to PyArrow Table. <code>to_duckdb_relation</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(query: str \\| None = None, reload: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DuckDB. <code>register_in_datafusion</code> <code>(query: str \\| None = None, reload: bool = False, ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DataFusion. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/base/#example_6","title":"Example","text":"<pre><code>from flowerpower_io import BaseDatabaseReader\n\n# Initialize database reader\nreader = BaseDatabaseReader(\n    type_=\"postgres\",\n    table_name=\"users\",\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n)\n\n# Query data\ndf = reader.to_polars(\"SELECT * FROM users WHERE active = true\")\n</code></pre>"},{"location":"api/base/#basedatabasewriter","title":"BaseDatabaseWriter","text":"<pre><code>class BaseDatabaseWriter(BaseDatabaseIO, gc=False)\n</code></pre> <p>The base class for database writing operations supporting various database systems.</p>"},{"location":"api/base/#parameters_7","title":"Parameters","text":"Parameter Type Description <code>mode</code> <code>str</code> Write mode (append, replace, fail). Default: \"append\" <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code>"},{"location":"api/base/#methods_7","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], mode: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None) -&gt; dict[str, Any]</code> Write data to database. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/base/#example_7","title":"Example","text":"<p>```python from flowerpower_io import BaseDatabaseWriter</p>"},{"location":"api/base/#initialize-database-writer","title":"Initialize database writer","text":"<p>writer = BaseDatabaseWriter(     type_=\"mysql\",     table_name=\"logs\",     host=\"localhost\",     username=\"user\",     password=\"password\",     database=\"appdb\",     mode=\"append\" )</p>"},{"location":"api/base/#write-data","title":"Write data","text":"<p>metadata = writer.write(df)</p>"},{"location":"api/loader/","title":"Loader Classes","text":"<p>The <code>flowerpower-io</code> library provides several loader classes for reading data from various sources and formats. These classes extend the base classes and provide specialized functionality for specific data sources.</p>"},{"location":"api/loader/#csv-loader","title":"CSV Loader","text":""},{"location":"api/loader/#csvloader","title":"CSVLoader","text":"<pre><code>class CSVLoader(BaseFileReader, gc=False)\n</code></pre> <p>Loader for CSV files.</p>"},{"location":"api/loader/#parameters","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to CSV file(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"csv\" <code>include_file_path</code> <code>bool</code> Include file path in the output DataFrame. Default: <code>False</code> <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>True</code> <code>batch_size</code> <code>int \\| None</code> Batch size for reading data. Default: <code>None</code> <code>opt_dtypes</code> <code>bool</code> Optimize data types. Default: <code>False</code> <code>use_threads</code> <code>bool</code> Use threads for reading data. Default: <code>True</code> <code>conn</code> <code>duckdb.DuckDBPyConnection \\| None</code> DuckDB connection instance. <code>ctx</code> <code>datafusion.SessionContext \\| None</code> DataFusion session context instance. <code>jsonlines</code> <code>bool \\| None</code> Whether the file is in JSON Lines format. Default: <code>False</code> <code>partitioning</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>verbose</code> <code>bool \\| None</code> Verbose output. <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>csv_options</code> <code>dict[str, Any]</code> Additional CSV reader options."},{"location":"api/loader/#methods","title":"Methods","text":"Method Signature Description <code>to_pandas</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame \\| list[pd.DataFrame], dict[str, Any]]</code> Convert data to Pandas DataFrame(s). <code>iter_pandas</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pd.DataFrame, None, None]</code> Iterate over Pandas DataFrames. <code>to_polars</code> <code>(lazy: bool = False, metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| tuple[pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame], dict[str, Any]]</code> Convert data to Polars DataFrame or LazyFrame. <code>iter_polars</code> <code>(lazy: bool = False, reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pl.DataFrame \\| pl.LazyFrame, None, None]</code> Iterate over Polars DataFrames or LazyFrames. <code>to_pyarrow_table</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pa.Table \\| list[pa.Table] \\| tuple[pa.Table \\| list[pa.Table], dict[str, Any]]</code> Convert data to PyArrow Table(s). <code>iter_pyarrow_table</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pa.Table, None, None]</code> Iterate over PyArrow Tables. <code>to_duckdb_relation</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Register data in DuckDB. <code>to_duckdb</code> <code>(as_relation: bool = True, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]] \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Convert data to DuckDB relation or register in DuckDB. <code>register_in_datafusion</code> <code>(ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; datafusion.SessionContext \\| tuple[datafusion.SessionContext, dict[str, Any]]</code> Register data in DataFusion. <code>filter</code> <code>(filter_expr: str \\| pl.Expr \\| pa.compute.Expression) -&gt; pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| list[pa.Table]</code> Filter data based on a filter expression. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/loader/#example","title":"Example","text":"<pre><code>from flowerpower_io.loader import CSVLoader\n\n# Initialize CSV loader\nloader = CSVLoader(\n    path=\"data/*.csv\",\n    include_file_path=True,\n    concat=True\n)\n\n# Convert to different formats\ndf_pandas = loader.to_pandas()\ndf_polars = loader.to_polars()\ntable_arrow = loader.to_pyarrow_table()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"my_table\")\n</code></pre>"},{"location":"api/loader/#parquet-loader","title":"Parquet Loader","text":""},{"location":"api/loader/#parquetloader","title":"ParquetLoader","text":"<pre><code>class ParquetLoader(BaseDatasetReader, gc=False)\n</code></pre> <p>Loader for Parquet files.</p>"},{"location":"api/loader/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to Parquet file(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"parquet\" <code>include_file_path</code> <code>bool</code> Include file path in the output DataFrame. Default: <code>False</code> <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>True</code> <code>batch_size</code> <code>int \\| None</code> Batch size for reading data. Default: <code>None</code> <code>opt_dtypes</code> <code>bool</code> Optimize data types. Default: <code>False</code> <code>use_threads</code> <code>bool</code> Use threads for reading data. Default: <code>True</code> <code>conn</code> <code>duckdb.DuckDBPyConnection \\| None</code> DuckDB connection instance. <code>ctx</code> <code>datafusion.SessionContext \\| None</code> DataFusion session context instance. <code>jsonlines</code> <code>bool \\| None</code> Whether the file is in JSON Lines format. Default: <code>False</code> <code>partitioning</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>verbose</code> <code>bool \\| None</code> Verbose output. <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>_dataset</code> <code>pds.Dataset \\| None</code> Internal PyArrow dataset. <code>_pydala_dataset</code> <code>Any \\| None</code> Internal Pydala dataset. <code>pydala</code> <code>bool</code> Use Pydala for reading. Default: <code>False</code>"},{"location":"api/loader/#methods_1","title":"Methods","text":"Method Signature Description <code>to_pandas</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame \\| list[pd.DataFrame], dict[str, Any]]</code> Convert data to Pandas DataFrame(s). <code>iter_pandas</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pd.DataFrame, None, None]</code> Iterate over Pandas DataFrames. <code>to_polars</code> <code>(lazy: bool = False, metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| tuple[pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame], dict[str, Any]]</code> Convert data to Polars DataFrame or LazyFrame. <code>iter_polars</code> <code>(lazy: bool = False, reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pl.DataFrame \\| pl.LazyFrame, None, None]</code> Iterate over Polars DataFrames or LazyFrames. <code>to_pyarrow_table</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pa.Table \\| list[pa.Table] \\| tuple[pa.Table \\| list[pa.Table], dict[str, Any]]</code> Convert data to PyArrow Table(s). <code>iter_pyarrow_table</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pa.Table, None, None]</code> Iterate over PyArrow Tables. <code>to_duckdb_relation</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Register data in DuckDB. <code>to_duckdb</code> <code>(as_relation: bool = True, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]] \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Convert data to DuckDB relation or register in DuckDB. <code>register_in_datafusion</code> <code>(ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; datafusion.SessionContext \\| tuple[datafusion.SessionContext, dict[str, Any]]</code> Register data in DataFusion. <code>filter</code> <code>(filter_expr: str \\| pl.Expr \\| pa.compute.Expression) -&gt; pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| list[pa.Table]</code> Filter data based on a filter expression. <code>metadata</code> <code>property</code> Get metadata for the loaded data. <code>to_pyarrow_dataset</code> <code>(metadata: bool = False, reload: bool = False, **kwargs) -&gt; pds.Dataset \\| tuple[pds.Dataset, dict[str, Any]]</code> Convert data to PyArrow Dataset. <code>to_pydala_dataset</code> <code>(metadata: bool = False, reload: bool = False, **kwargs) -&gt; ParquetDataset \\| tuple[ParquetDataset, dict[str, Any]]</code> Convert data to Pydala ParquetDataset."},{"location":"api/loader/#example_1","title":"Example","text":"<pre><code>from flowerpower_io.loader import ParquetLoader\n\n# Initialize Parquet loader\nloader = ParquetLoader(\n    path=\"data/*.parquet\",\n    include_file_path=True,\n    concat=True\n)\n\n# Convert to different formats\ndf_pandas = loader.to_pandas()\ndf_polars = loader.to_polars()\ntable_arrow = loader.to_pyarrow_table()\ndataset = loader.to_pyarrow_dataset()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"my_table\")\n</code></pre>"},{"location":"api/loader/#json-loader","title":"JSON Loader","text":""},{"location":"api/loader/#jsonloader","title":"JSONLoader","text":"<pre><code>class JSONLoader(BaseFileReader, gc=False)\n</code></pre> <p>Loader for JSON files.</p>"},{"location":"api/loader/#parameters_2","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to JSON file(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"json\" <code>include_file_path</code> <code>bool</code> Include file path in the output DataFrame. Default: <code>False</code> <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>True</code> <code>batch_size</code> <code>int \\| None</code> Batch size for reading data. Default: <code>None</code> <code>opt_dtypes</code> <code>bool</code> Optimize data types. Default: <code>False</code> <code>use_threads</code> <code>bool</code> Use threads for reading data. Default: <code>True</code> <code>conn</code> <code>duckdb.DuckDBPyConnection \\| None</code> DuckDB connection instance. <code>ctx</code> <code>datafusion.SessionContext \\| None</code> DataFusion session context instance. <code>jsonlines</code> <code>bool \\| None</code> Whether the file is in JSON Lines format. Default: <code>False</code> <code>partitioning</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>verbose</code> <code>bool \\| None</code> Verbose output. <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>json_options</code> <code>dict[str, Any]</code> Additional JSON reader options."},{"location":"api/loader/#methods_2","title":"Methods","text":"Method Signature Description <code>to_pandas</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame \\| list[pd.DataFrame], dict[str, Any]]</code> Convert data to Pandas DataFrame(s). <code>iter_pandas</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pd.DataFrame, None, None]</code> Iterate over Pandas DataFrames. <code>to_polars</code> <code>(lazy: bool = False, metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| tuple[pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame], dict[str, Any]]</code> Convert data to Polars DataFrame or LazyFrame. <code>iter_polars</code> <code>(lazy: bool = False, reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pl.DataFrame \\| pl.LazyFrame, None, None]</code> Iterate over Polars DataFrames or LazyFrames. <code>to_pyarrow_table</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pa.Table \\| list[pa.Table] \\| tuple[pa.Table \\| list[pa.Table], dict[str, Any]]</code> Convert data to PyArrow Table(s). <code>iter_pyarrow_table</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pa.Table, None, None]</code> Iterate over PyArrow Tables. <code>to_duckdb_relation</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Register data in DuckDB. <code>to_duckdb</code> <code>(as_relation: bool = True, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]] \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Convert data to DuckDB relation or register in DuckDB. <code>register_in_datafusion</code> <code>(ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; datafusion.SessionContext \\| tuple[datafusion.SessionContext, dict[str, Any]]</code> Register data in DataFusion. <code>filter</code> <code>(filter_expr: str \\| pl.Expr \\| pa.compute.Expression) -&gt; pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| list[pa.Table]</code> Filter data based on a filter expression. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/loader/#example_2","title":"Example","text":"<pre><code>from flowerpower_io.loader import JSONLoader\n\n# Initialize JSON loader\nloader = JSONLoader(\n    path=\"data/*.json\",\n    include_file_path=True,\n    concat=True\n)\n\n# Convert to different formats\ndf_pandas = loader.to_pandas()\ndf_polars = loader.to_polars()\ntable_arrow = loader.to_pyarrow_table()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"my_table\")\n</code></pre>"},{"location":"api/loader/#deltatable-loader","title":"DeltaTable Loader","text":""},{"location":"api/loader/#deltatableloader","title":"DeltaTableLoader","text":"<pre><code>class DeltaTableLoader(BaseDatasetReader, gc=False)\n</code></pre> <p>Loader for Delta Lake tables.</p>"},{"location":"api/loader/#parameters_3","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to Delta Lake table(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"deltatable\" <code>include_file_path</code> <code>bool</code> Include file path in the output DataFrame. Default: <code>False</code> <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>True</code> <code>batch_size</code> <code>int \\| None</code> Batch size for reading data. Default: <code>None</code> <code>opt_dtypes</code> <code>bool</code> Optimize data types. Default: <code>False</code> <code>use_threads</code> <code>bool</code> Use threads for reading data. Default: <code>True</code> <code>conn</code> <code>duckdb.DuckDBPyConnection \\| None</code> DuckDB connection instance. <code>ctx</code> <code>datafusion.SessionContext \\| None</code> DataFusion session context instance. <code>jsonlines</code> <code>bool \\| None</code> Whether the file is in JSON Lines format. Default: <code>False</code> <code>partitioning</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>verbose</code> <code>bool \\| None</code> Verbose output. <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>_dataset</code> <code>pds.Dataset \\| None</code> Internal PyArrow dataset. <code>_pydala_dataset</code> <code>Any \\| None</code> Internal Pydala dataset. <code>delta_options</code> <code>dict[str, Any]</code> Additional Delta Lake reader options."},{"location":"api/loader/#methods_3","title":"Methods","text":"Method Signature Description <code>to_pandas</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame \\| list[pd.DataFrame], dict[str, Any]]</code> Convert data to Pandas DataFrame(s). <code>iter_pandas</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pd.DataFrame, None, None]</code> Iterate over Pandas DataFrames. <code>to_polars</code> <code>(lazy: bool = False, metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| tuple[pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame], dict[str, Any]]</code> Convert data to Polars DataFrame or LazyFrame. <code>iter_polars</code> <code>(lazy: bool = False, reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pl.DataFrame \\| pl.LazyFrame, None, None]</code> Iterate over Polars DataFrames or LazyFrames. <code>to_pyarrow_table</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pa.Table \\| list[pa.Table] \\| tuple[pa.Table \\| list[pa.Table], dict[str, Any]]</code> Convert data to PyArrow Table(s). <code>iter_pyarrow_table</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pa.Table, None, None]</code> Iterate over PyArrow Tables. <code>to_duckdb_relation</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Register data in DuckDB. <code>to_duckdb</code> <code>(as_relation: bool = True, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]] \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Convert data to DuckDB relation or register in DuckDB. <code>register_in_datafusion</code> <code>(ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; datafusion.SessionContext \\| tuple[datafusion.SessionContext, dict[str, Any]]</code> Register data in DataFusion. <code>filter</code> <code>(filter_expr: str \\| pl.Expr \\| pa.compute.Expression) -&gt; pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| list[pa.Table]</code> Filter data based on a filter expression. <code>metadata</code> <code>property</code> Get metadata for the loaded data. <code>to_pyarrow_dataset</code> <code>(metadata: bool = False, reload: bool = False, **kwargs) -&gt; pds.Dataset \\| tuple[pds.Dataset, dict[str, Any]]</code> Convert data to PyArrow Dataset. <code>to_pydala_dataset</code> <code>(metadata: bool = False, reload: bool = False, **kwargs) -&gt; ParquetDataset \\| tuple[ParquetDataset, dict[str, Any]]</code> Convert data to Pydala ParquetDataset."},{"location":"api/loader/#example_3","title":"Example","text":"<pre><code>from flowerpower_io.loader import DeltaTableLoader\n\n# Initialize DeltaTable loader\nloader = DeltaTableLoader(\n    path=\"delta_table/\",\n    include_file_path=True,\n    concat=True\n)\n\n# Convert to different formats\ndf_pandas = loader.to_pandas()\ndf_polars = loader.to_polars()\ntable_arrow = loader.to_pyarrow_table()\ndataset = loader.to_pyarrow_dataset()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"my_table\")\n</code></pre>"},{"location":"api/loader/#pydala-loader","title":"Pydala Loader","text":""},{"location":"api/loader/#pydalaloader","title":"PydalaLoader","text":"<pre><code>class PydalaLoader(BaseDatasetReader, gc=False)\n</code></pre> <p>Loader for Pydala datasets.</p>"},{"location":"api/loader/#parameters_4","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to Pydala dataset(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"pydala\" <code>include_file_path</code> <code>bool</code> Include file path in the output DataFrame. Default: <code>False</code> <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>True</code> <code>batch_size</code> <code>int \\| None</code> Batch size for reading data. Default: <code>None</code> <code>opt_dtypes</code> <code>bool</code> Optimize data types. Default: <code>False</code> <code>use_threads</code> <code>bool</code> Use threads for reading data. Default: <code>True</code> <code>conn</code> <code>duckdb.DuckDBPyConnection \\| None</code> DuckDB connection instance. <code>ctx</code> <code>datafusion.SessionContext \\| None</code> DataFusion session context instance. <code>jsonlines</code> <code>bool \\| None</code> Whether the file is in JSON Lines format. Default: <code>False</code> <code>partitioning</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>verbose</code> <code>bool \\| None</code> Verbose output. <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>_dataset</code> <code>pds.Dataset \\| None</code> Internal PyArrow dataset. <code>_pydala_dataset</code> <code>Any \\| None</code> Internal Pydala dataset. <code>pydala_options</code> <code>dict[str, Any]</code> Additional Pydala reader options."},{"location":"api/loader/#methods_4","title":"Methods","text":"Method Signature Description <code>to_pandas</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame \\| list[pd.DataFrame], dict[str, Any]]</code> Convert data to Pandas DataFrame(s). <code>iter_pandas</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pd.DataFrame, None, None]</code> Iterate over Pandas DataFrames. <code>to_polars</code> <code>(lazy: bool = False, metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| tuple[pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame], dict[str, Any]]</code> Convert data to Polars DataFrame or LazyFrame. <code>iter_polars</code> <code>(lazy: bool = False, reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pl.DataFrame \\| pl.LazyFrame, None, None]</code> Iterate over Polars DataFrames or LazyFrames. <code>to_pyarrow_table</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pa.Table \\| list[pa.Table] \\| tuple[pa.Table \\| list[pa.Table], dict[str, Any]]</code> Convert data to PyArrow Table(s). <code>iter_pyarrow_table</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pa.Table, None, None]</code> Iterate over PyArrow Tables. <code>to_duckdb_relation</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Register data in DuckDB. <code>to_duckdb</code> <code>(as_relation: bool = True, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]] \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Convert data to DuckDB relation or register in DuckDB. <code>register_in_datafusion</code> <code>(ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; datafusion.SessionContext \\| tuple[datafusion.SessionContext, dict[str, Any]]</code> Register data in DataFusion. <code>filter</code> <code>(filter_expr: str \\| pl.Expr \\| pa.compute.Expression) -&gt; pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| list[pa.Table]</code> Filter data based on a filter expression. <code>metadata</code> <code>property</code> Get metadata for the loaded data. <code>to_pyarrow_dataset</code> <code>(metadata: bool = False, reload: bool = False, **kwargs) -&gt; pds.Dataset \\| tuple[pds.Dataset, dict[str, Any]]</code> Convert data to PyArrow Dataset. <code>to_pydala_dataset</code> <code>(metadata: bool = False, reload: bool = False, **kwargs) -&gt; ParquetDataset \\| tuple[ParquetDataset, dict[str, Any]]</code> Convert data to Pydala ParquetDataset."},{"location":"api/loader/#example_4","title":"Example","text":"<pre><code>from flowerpower_io.loader import PydalaLoader\n\n# Initialize Pydala loader\nloader = PydalaLoader(\n    path=\"pydala_dataset/\",\n    include_file_path=True,\n    concat=True\n)\n\n# Convert to different formats\ndf_pandas = loader.to_pandas()\ndf_polars = loader.to_polars()\ntable_arrow = loader.to_pyarrow_table()\ndataset = loader.to_pyarrow_dataset()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"my_table\")\n</code></pre>"},{"location":"api/loader/#mqtt-loader","title":"MQTT Loader","text":""},{"location":"api/loader/#mqttloader","title":"MQTTLoader","text":"<pre><code>class MQTTLoader(BaseFileReader, gc=False)\n</code></pre> <p>Loader for MQTT messages.</p>"},{"location":"api/loader/#parameters_5","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> MQTT topic(s) to subscribe to. <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> MQTT connection options. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"mqtt\" <code>include_file_path</code> <code>bool</code> Include file path in the output DataFrame. Default: <code>False</code> <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>True</code> <code>batch_size</code> <code>int \\| None</code> Batch size for reading data. Default: <code>None</code> <code>opt_dtypes</code> <code>bool</code> Optimize data types. Default: <code>False</code> <code>use_threads</code> <code>bool</code> Use threads for reading data. Default: <code>True</code> <code>conn</code> <code>duckdb.DuckDBPyConnection \\| None</code> DuckDB connection instance. <code>ctx</code> <code>datafusion.SessionContext \\| None</code> DataFusion session context instance. <code>jsonlines</code> <code>bool \\| None</code> Whether the file is in JSON Lines format. Default: <code>False</code> <code>partitioning</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>verbose</code> <code>bool \\| None</code> Verbose output. <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>mqtt_options</code> <code>dict[str, Any]</code> Additional MQTT reader options."},{"location":"api/loader/#methods_5","title":"Methods","text":"Method Signature Description <code>to_pandas</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame \\| list[pd.DataFrame], dict[str, Any]]</code> Convert data to Pandas DataFrame(s). <code>iter_pandas</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pd.DataFrame, None, None]</code> Iterate over Pandas DataFrames. <code>to_polars</code> <code>(lazy: bool = False, metadata: bool = False, reload: bool = False, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| tuple[pl.DataFrame \\| pl.LazyFrame \\| list[pl.DataFrame] \\| list[pl.LazyFrame], dict[str, Any]]</code> Convert data to Polars DataFrame or LazyFrame. <code>iter_polars</code> <code>(lazy: bool = False, reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pl.DataFrame \\| pl.LazyFrame, None, None]</code> Iterate over Polars DataFrames or LazyFrames. <code>to_pyarrow_table</code> <code>(metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; pa.Table \\| list[pa.Table] \\| tuple[pa.Table \\| list[pa.Table], dict[str, Any]]</code> Convert data to PyArrow Table(s). <code>iter_pyarrow_table</code> <code>(reload: bool = False, batch_size: int \\| None = None, include_file_path: bool = False, concat: bool \\| None = None, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; Generator[pa.Table, None, None]</code> Iterate over PyArrow Tables. <code>to_duckdb_relation</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Register data in DuckDB. <code>to_duckdb</code> <code>(as_relation: bool = True, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| duckdb.DuckDBPyConnection \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]] \\| tuple[duckdb.DuckDBPyConnection, dict[str, Any]]</code> Convert data to DuckDB relation or register in DuckDB. <code>register_in_datafusion</code> <code>(ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, metadata: bool = False, reload: bool = False, include_file_path: bool = False, use_threads: bool \\| None = None, verbose: bool \\| None = None, opt_dtypes: bool \\| None = None, **kwargs) -&gt; datafusion.SessionContext \\| tuple[datafusion.SessionContext, dict[str, Any]]</code> Register data in DataFusion. <code>filter</code> <code>(filter_expr: str \\| pl.Expr \\| pa.compute.Expression) -&gt; pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| list[pl.DataFrame] \\| list[pl.LazyFrame] \\| list[pa.Table]</code> Filter data based on a filter expression. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/loader/#example_5","title":"Example","text":"<pre><code>from flowerpower_io.loader import MQTTLoader\n\n# Initialize MQTT loader\nloader = MQTTLoader(\n    path=\"sensors/temperature\",\n    storage_options={\n        \"host\": \"mqtt.example.com\",\n        \"port\": 1883,\n        \"username\": \"user\",\n        \"password\": \"password\"\n    }\n)\n\n# Convert to different formats\ndf_pandas = loader.to_pandas()\ndf_polars = loader.to_polars()\ntable_arrow = loader.to_pyarrow_table()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"temperature_data\")\n</code></pre>"},{"location":"api/loader/#database-loaders","title":"Database Loaders","text":""},{"location":"api/loader/#sqliteloader","title":"SQLiteLoader","text":"<pre><code>class SQLiteLoader(BaseDatabaseReader, gc=False)\n</code></pre> <p>Loader for SQLite databases.</p>"},{"location":"api/loader/#parameters_6","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"sqlite\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite database. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>query</code> <code>str \\| None</code> SQL query to execute. <code>sqlite_options</code> <code>dict[str, Any]</code> Additional SQLite reader options."},{"location":"api/loader/#methods_6","title":"Methods","text":"Method Signature Description <code>to_polars</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pl.DataFrame \\| tuple[pl.DataFrame, dict[str, Any]]</code> Convert data to Polars DataFrame. <code>to_pandas</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame, dict[str, Any]]</code> Convert data to Pandas DataFrame. <code>to_pyarrow_table</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pa.Table \\| tuple[pa.Table, dict[str, Any]]</code> Convert data to PyArrow Table. <code>to_duckdb_relation</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(query: str \\| None = None, reload: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DuckDB. <code>register_in_datafusion</code> <code>(query: str \\| None = None, reload: bool = False, ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DataFusion. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/loader/#example_6","title":"Example","text":"<pre><code>from flowerpower_io.loader import SQLiteLoader\n\n# Initialize SQLite loader\nloader = SQLiteLoader(\n    type_=\"sqlite\",\n    table_name=\"users\",\n    path=\"database.db\"\n)\n\n# Convert to different formats\ndf_polars = loader.to_polars()\ndf_pandas = loader.to_pandas()\ntable_arrow = loader.to_pyarrow_table()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"users\")\n</code></pre>"},{"location":"api/loader/#duckdbloader","title":"DuckDBLoader","text":"<pre><code>class DuckDBLoader(BaseDatabaseReader, gc=False)\n</code></pre> <p>Loader for DuckDB databases.</p>"},{"location":"api/loader/#parameters_7","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"duckdb\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for DuckDB database. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>query</code> <code>str \\| None</code> SQL query to execute. <code>duckdb_options</code> <code>dict[str, Any]</code> Additional DuckDB reader options."},{"location":"api/loader/#methods_7","title":"Methods","text":"Method Signature Description <code>to_polars</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pl.DataFrame \\| tuple[pl.DataFrame, dict[str, Any]]</code> Convert data to Polars DataFrame. <code>to_pandas</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame, dict[str, Any]]</code> Convert data to Pandas DataFrame. <code>to_pyarrow_table</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pa.Table \\| tuple[pa.Table, dict[str, Any]]</code> Convert data to PyArrow Table. <code>to_duckdb_relation</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(query: str \\| None = None, reload: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DuckDB. <code>register_in_datafusion</code> <code>(query: str \\| None = None, reload: bool = False, ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DataFusion. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/loader/#example_7","title":"Example","text":"<pre><code>from flowerpower_io.loader import DuckDBLoader\n\n# Initialize DuckDB loader\nloader = DuckDBLoader(\n    type_=\"duckdb\",\n    table_name=\"users\",\n    path=\"database.db\"\n)\n\n# Convert to different formats\ndf_polars = loader.to_polars()\ndf_pandas = loader.to_pandas()\ntable_arrow = loader.to_pyarrow_table()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"users\")\n</code></pre>"},{"location":"api/loader/#postgresqlloader","title":"PostgreSQLLoader","text":"<pre><code>class PostgreSQLLoader(BaseDatabaseReader, gc=False)\n</code></pre> <p>Loader for PostgreSQL databases.</p>"},{"location":"api/loader/#parameters_8","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"postgres\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite or DuckDB databases. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>query</code> <code>str \\| None</code> SQL query to execute. <code>postgres_options</code> <code>dict[str, Any]</code> Additional PostgreSQL reader options."},{"location":"api/loader/#methods_8","title":"Methods","text":"Method Signature Description <code>to_polars</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pl.DataFrame \\| tuple[pl.DataFrame, dict[str, Any]]</code> Convert data to Polars DataFrame. <code>to_pandas</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame, dict[str, Any]]</code> Convert data to Pandas DataFrame. <code>to_pyarrow_table</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pa.Table \\| tuple[pa.Table, dict[str, Any]]</code> Convert data to PyArrow Table. <code>to_duckdb_relation</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(query: str \\| None = None, reload: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DuckDB. <code>register_in_datafusion</code> <code>(query: str \\| None = None, reload: bool = False, ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DataFusion. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/loader/#example_8","title":"Example","text":"<pre><code>from flowerpower_io.loader import PostgreSQLLoader\n\n# Initialize PostgreSQL loader\nloader = PostgreSQLLoader(\n    type_=\"postgres\",\n    table_name=\"users\",\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n)\n\n# Convert to different formats\ndf_polars = loader.to_polars()\ndf_pandas = loader.to_pandas()\ntable_arrow = loader.to_pyarrow_table()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"users\")\n</code></pre>"},{"location":"api/loader/#mysqlloader","title":"MySQLLoader","text":"<pre><code>class MySQLLoader(BaseDatabaseReader, gc=False)\n</code></pre> <p>Loader for MySQL databases.</p>"},{"location":"api/loader/#parameters_9","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"mysql\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite or DuckDB databases. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>query</code> <code>str \\| None</code> SQL query to execute. <code>mysql_options</code> <code>dict[str, Any]</code> Additional MySQL reader options."},{"location":"api/loader/#methods_9","title":"Methods","text":"Method Signature Description <code>to_polars</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pl.DataFrame \\| tuple[pl.DataFrame, dict[str, Any]]</code> Convert data to Polars DataFrame. <code>to_pandas</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame, dict[str, Any]]</code> Convert data to Pandas DataFrame. <code>to_pyarrow_table</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pa.Table \\| tuple[pa.Table, dict[str, Any]]</code> Convert data to PyArrow Table. <code>to_duckdb_relation</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(query: str \\| None = None, reload: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DuckDB. <code>register_in_datafusion</code> <code>(query: str \\| None = None, reload: bool = False, ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DataFusion. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/loader/#example_9","title":"Example","text":"<pre><code>from flowerpower_io.loader import MySQLLoader\n\n# Initialize MySQL loader\nloader = MySQLLoader(\n    type_=\"mysql\",\n    table_name=\"users\",\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n)\n\n# Convert to different formats\ndf_polars = loader.to_polars()\ndf_pandas = loader.to_pandas()\ntable_arrow = loader.to_pyarrow_table()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"users\")\n</code></pre>"},{"location":"api/loader/#mssqlloader","title":"MSSQLLoader","text":"<pre><code>class MSSQLLoader(BaseDatabaseReader, gc=False)\n</code></pre> <p>Loader for Microsoft SQL Server databases.</p>"},{"location":"api/loader/#parameters_10","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"mssql\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite or DuckDB databases. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>query</code> <code>str \\| None</code> SQL query to execute. <code>mssql_options</code> <code>dict[str, Any]</code> Additional MSSQL reader options."},{"location":"api/loader/#methods_10","title":"Methods","text":"Method Signature Description <code>to_polars</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pl.DataFrame \\| tuple[pl.DataFrame, dict[str, Any]]</code> Convert data to Polars DataFrame. <code>to_pandas</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame, dict[str, Any]]</code> Convert data to Pandas DataFrame. <code>to_pyarrow_table</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pa.Table \\| tuple[pa.Table, dict[str, Any]]</code> Convert data to PyArrow Table. <code>to_duckdb_relation</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(query: str \\| None = None, reload: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DuckDB. <code>register_in_datafusion</code> <code>(query: str \\| None = None, reload: bool = False, ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DataFusion. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/loader/#example_10","title":"Example","text":"<pre><code>from flowerpower_io.loader import MSSQLLoader\n\n# Initialize MSSQL loader\nloader = MSSQLLoader(\n    type_=\"mssql\",\n    table_name=\"users\",\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n)\n\n# Convert to different formats\ndf_polars = loader.to_polars()\ndf_pandas = loader.to_pandas()\ntable_arrow = loader.to_pyarrow_table()\n\n# Register in DuckDB\nconn = loader.register_in_duckdb(name=\"users\")\n</code></pre>"},{"location":"api/loader/#oracleloader","title":"OracleLoader","text":"<pre><code>class OracleLoader(BaseDatabaseReader, gc=False)\n</code></pre> <p>Loader for Oracle databases.</p>"},{"location":"api/loader/#parameters_11","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"oracle\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite or DuckDB databases. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>query</code> <code>str \\| None</code> SQL query to execute. <code>oracle_options</code> <code>dict[str, Any]</code> Additional Oracle reader options."},{"location":"api/loader/#methods_11","title":"Methods","text":"Method Signature Description <code>to_polars</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pl.DataFrame \\| tuple[pl.DataFrame, dict[str, Any]]</code> Convert data to Polars DataFrame. <code>to_pandas</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pd.DataFrame \\| tuple[pd.DataFrame, dict[str, Any]]</code> Convert data to Pandas DataFrame. <code>to_pyarrow_table</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, **kwargs) -&gt; pa.Table \\| tuple[pa.Table, dict[str, Any]]</code> Convert data to PyArrow Table. <code>to_duckdb_relation</code> <code>(query: str \\| None = None, reload: bool = False, metadata: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, **kwargs) -&gt; duckdb.DuckDBPyRelation \\| tuple[duckdb.DuckDBPyRelation, dict[str, Any]]</code> Convert data to DuckDB relation. <code>register_in_duckdb</code> <code>(query: str \\| None = None, reload: bool = False, conn: duckdb.DuckDBPyConnection \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DuckDB. <code>register_in_datafusion</code> <code>(query: str \\| None = None, reload: bool = False, ctx: datafusion.SessionContext \\| None = None, name: str \\| None = None, **kwargs) -&gt; None</code> Register data in DataFusion. <code>metadata</code> <code>property</code> Get metadata for the loaded data."},{"location":"api/loader/#example_11","title":"Example","text":"<p>```python from flowerpower_io.loader import OracleLoader</p>"},{"location":"api/loader/#initialize-oracle-loader","title":"Initialize Oracle loader","text":"<p>loader = OracleLoader(     type_=\"oracle\",     table_name=\"users\",     host=\"localhost\",     username=\"user\",     password=\"password\",     database=\"mydb\" )</p>"},{"location":"api/loader/#convert-to-different-formats","title":"Convert to different formats","text":"<p>df_polars = loader.to_polars() df_pandas = loader.to_pandas() table_arrow = loader.to_pyarrow_table()</p>"},{"location":"api/loader/#register-in-duckdb","title":"Register in DuckDB","text":"<p>conn = loader.register_in_duckdb(name=\"users\")</p>"},{"location":"api/metadata/","title":"Metadata Functions","text":"<p>The <code>flowerpower-io</code> library provides several functions for extracting metadata from various data sources. These functions help you understand the structure, statistics, and properties of your data before processing it.</p>"},{"location":"api/metadata/#get_dataframe_metadata","title":"get_dataframe_metadata","text":"<pre><code>def get_dataframe_metadata(\n    df: pl.DataFrame | pl.LazyFrame | pd.DataFrame,\n    include_stats: bool = True,\n    include_schema: bool = True,\n    include_null_counts: bool = True,\n    include_memory_usage: bool = True,\n    include_duplicates: bool = True,\n    include_sample: bool = True,\n    sample_size: int = 5,\n    verbose: bool = False\n) -&gt; dict[str, Any]\n</code></pre> <p>Extract comprehensive metadata from a Polars or Pandas DataFrame.</p>"},{"location":"api/metadata/#parameters","title":"Parameters","text":"Parameter Type Description <code>df</code> <code>pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame</code> Input DataFrame to analyze. <code>include_stats</code> <code>bool</code> Include basic statistics (min, max, mean, etc.). Default: <code>True</code> <code>include_schema</code> <code>bool</code> Include schema information (data types). Default: <code>True</code> <code>include_null_counts</code> <code>bool</code> Include null value counts. Default: <code>True</code> <code>include_memory_usage</code> <code>bool</code> Include memory usage information. Default: <code>True</code> <code>include_duplicates</code> <code>bool</code> Include duplicate row count. Default: <code>True</code> <code>include_sample</code> <code>bool</code> Include sample data. Default: <code>True</code> <code>sample_size</code> <code>int</code> Number of rows to include in sample. Default: <code>5</code> <code>verbose</code> <code>bool</code> Enable verbose output. Default: <code>False</code>"},{"location":"api/metadata/#returns","title":"Returns","text":"Type Description <code>dict[str, Any]</code> Dictionary containing metadata about the DataFrame."},{"location":"api/metadata/#example","title":"Example","text":"<pre><code>import polars as pl\nfrom flowerpower_io.metadata import get_dataframe_metadata\n\n# Create a sample DataFrame\ndf = pl.DataFrame({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n    \"age\": [25, 30, 35, 40, 45],\n    \"salary\": [50000, 60000, 70000, 80000, 90000]\n})\n\n# Extract metadata\nmetadata = get_dataframe_metadata(df)\nprint(metadata)\n</code></pre>"},{"location":"api/metadata/#get_pyarrow_table_metadata","title":"get_pyarrow_table_metadata","text":"<pre><code>def get_pyarrow_table_metadata(\n    table: pa.Table,\n    include_stats: bool = True,\n    include_schema: bool = True,\n    include_null_counts: bool = True,\n    include_memory_usage: bool = True,\n    include_duplicates: bool = True,\n    include_sample: bool = True,\n    sample_size: int = 5,\n    verbose: bool = False\n) -&gt; dict[str, Any]\n</code></pre> <p>Extract comprehensive metadata from a PyArrow Table.</p>"},{"location":"api/metadata/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>table</code> <code>pa.Table</code> Input PyArrow Table to analyze. <code>include_stats</code> <code>bool</code> Include basic statistics (min, max, mean, etc.). Default: <code>True</code> <code>include_schema</code> <code>bool</code> Include schema information (data types). Default: <code>True</code> <code>include_null_counts</code> <code>bool</code> Include null value counts. Default: <code>True</code> <code>include_memory_usage</code> <code>bool</code> Include memory usage information. Default: <code>True</code> <code>include_duplicates</code> <code>bool</code> Include duplicate row count. Default: <code>True</code> <code>include_sample</code> <code>bool</code> Include sample data. Default: <code>True</code> <code>sample_size</code> <code>int</code> Number of rows to include in sample. Default: <code>5</code> <code>verbose</code> <code>bool</code> Enable verbose output. Default: <code>False</code>"},{"location":"api/metadata/#returns_1","title":"Returns","text":"Type Description <code>dict[str, Any]</code> Dictionary containing metadata about the PyArrow Table."},{"location":"api/metadata/#example_1","title":"Example","text":"<pre><code>import pyarrow as pa\nfrom flowerpower_io.metadata import get_pyarrow_table_metadata\n\n# Create a sample PyArrow Table\ntable = pa.table({\n    \"id\": [1, 2, 3, 4, 5],\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n    \"age\": [25, 30, 35, 40, 45],\n    \"salary\": [50000, 60000, 70000, 80000, 90000]\n})\n\n# Extract metadata\nmetadata = get_pyarrow_table_metadata(table)\nprint(metadata)\n</code></pre>"},{"location":"api/metadata/#get_pyarrow_dataset_metadata","title":"get_pyarrow_dataset_metadata","text":"<pre><code>def get_pyarrow_dataset_metadata(\n    dataset: pds.Dataset,\n    include_stats: bool = True,\n    include_schema: bool = True,\n    include_null_counts: bool = True,\n    include_memory_usage: bool = True,\n    include_duplicates: bool = True,\n    include_sample: bool = True,\n    sample_size: int = 5,\n    verbose: bool = False\n) -&gt; dict[str, Any]\n</code></pre> <p>Extract comprehensive metadata from a PyArrow Dataset.</p>"},{"location":"api/metadata/#parameters_2","title":"Parameters","text":"Parameter Type Description <code>dataset</code> <code>pds.Dataset</code> Input PyArrow Dataset to analyze. <code>include_stats</code> <code>bool</code> Include basic statistics (min, max, mean, etc.). Default: <code>True</code> <code>include_schema</code> <code>bool</code> Include schema information (data types). Default: <code>True</code> <code>include_null_counts</code> <code>bool</code> Include null value counts. Default: <code>True</code> <code>include_memory_usage</code> <code>bool</code> Include memory usage information. Default: <code>True</code> <code>include_duplicates</code> <code>bool</code> Include duplicate row count. Default: <code>True</code> <code>include_sample</code> <code>bool</code> Include sample data. Default: <code>True</code> <code>sample_size</code> <code>int</code> Number of rows to include in sample. Default: <code>5</code> <code>verbose</code> <code>bool</code> Enable verbose output. Default: <code>False</code>"},{"location":"api/metadata/#returns_2","title":"Returns","text":"Type Description <code>dict[str, Any]</code> Dictionary containing metadata about the PyArrow Dataset."},{"location":"api/metadata/#example_2","title":"Example","text":"<pre><code>import pyarrow.dataset as pds\nfrom flowerpower_io.metadata import get_pyarrow_dataset_metadata\n\n# Create a sample PyArrow Dataset\ndataset = pds.dataset(\"data/\")\n\n# Extract metadata\nmetadata = get_pyarrow_dataset_metadata(dataset)\nprint(metadata)\n</code></pre>"},{"location":"api/metadata/#get_duckdb_relation_metadata","title":"get_duckdb_relation_metadata","text":"<pre><code>def get_duckdb_relation_metadata(\n    relation: duckdb.DuckDBPyRelation,\n    include_stats: bool = True,\n    include_schema: bool = True,\n    include_null_counts: bool = True,\n    include_memory_usage: bool = True,\n    include_duplicates: bool = True,\n    include_sample: bool = True,\n    sample_size: int = 5,\n    verbose: bool = False\n) -&gt; dict[str, Any]\n</code></pre> <p>Extract comprehensive metadata from a DuckDB relation.</p>"},{"location":"api/metadata/#parameters_3","title":"Parameters","text":"Parameter Type Description <code>relation</code> <code>duckdb.DuckDBPyRelation</code> Input DuckDB relation to analyze. <code>include_stats</code> <code>bool</code> Include basic statistics (min, max, mean, etc.). Default: <code>True</code> <code>include_schema</code> <code>bool</code> Include schema information (data types). Default: <code>True</code> <code>include_null_counts</code> <code>bool</code> Include null value counts. Default: <code>True</code> <code>include_memory_usage</code> <code>bool</code> Include memory usage information. Default: <code>True</code> <code>include_duplicates</code> <code>bool</code> Include duplicate row count. Default: <code>True</code> <code>include_sample</code> <code>bool</code> Include sample data. Default: <code>True</code> <code>sample_size</code> <code>int</code> Number of rows to include in sample. Default: <code>5</code> <code>verbose</code> <code>bool</code> Enable verbose output. Default: <code>False</code>"},{"location":"api/metadata/#returns_3","title":"Returns","text":"Type Description <code>dict[str, Any]</code> Dictionary containing metadata about the DuckDB relation."},{"location":"api/metadata/#example_3","title":"Example","text":"<pre><code>import duckdb\nfrom flowerpower_io.metadata import get_duckdb_relation_metadata\n\n# Create a sample DuckDB relation\nconn = duckdb.connect()\nrelation = conn.execute(\"SELECT * FROM (VALUES (1, 'Alice'), (2, 'Bob'), (3, 'Charlie')) AS t(id, name)\")\n\n# Extract metadata\nmetadata = get_duckdb_relation_metadata(relation)\nprint(metadata)\n</code></pre>"},{"location":"api/metadata/#get_datafusion_relation_metadata","title":"get_datafusion_relation_metadata","text":"<pre><code>def get_datafusion_relation_metadata(\n    relation: datafusion.DataFrame,\n    include_stats: bool = True,\n    include_schema: bool = True,\n    include_null_counts: bool = True,\n    include_memory_usage: bool = True,\n    include_duplicates: bool = True,\n    include_sample: bool = True,\n    sample_size: int = 5,\n    verbose: bool = False\n) -&gt; dict[str, Any]\n</code></pre> <p>Extract comprehensive metadata from a DataFusion DataFrame.</p>"},{"location":"api/metadata/#parameters_4","title":"Parameters","text":"Parameter Type Description <code>relation</code> <code>datafusion.DataFrame</code> Input DataFusion DataFrame to analyze. <code>include_stats</code> <code>bool</code> Include basic statistics (min, max, mean, etc.). Default: <code>True</code> <code>include_schema</code> <code>bool</code> Include schema information (data types). Default: <code>True</code> <code>include_null_counts</code> <code>bool</code> Include null value counts. Default: <code>True</code> <code>include_memory_usage</code> <code>bool</code> Include memory usage information. Default: <code>True</code> <code>include_duplicates</code> <code>bool</code> Include duplicate row count. Default: <code>True</code> <code>include_sample</code> <code>bool</code> Include sample data. Default: <code>True</code> <code>sample_size</code> <code>int</code> Number of rows to include in sample. Default: <code>5</code> <code>verbose</code> <code>bool</code> Enable verbose output. Default: <code>False</code>"},{"location":"api/metadata/#returns_4","title":"Returns","text":"Type Description <code>dict[str, Any]</code> Dictionary containing metadata about the DataFusion DataFrame."},{"location":"api/metadata/#example_4","title":"Example","text":"<pre><code>import datafusion\nfrom flowerpower_io.metadata import get_datafusion_relation_metadata\n\n# Create a sample DataFusion DataFrame\nctx = datafusion.SessionContext()\ndf = ctx.sql(\"SELECT * FROM (VALUES (1, 'Alice'), (2, 'Bob'), (3, 'Charlie')) AS t(id, name)\")\n\n# Extract metadata\nmetadata = get_datafusion_relation_metadata(df)\nprint(metadata)\n</code></pre>"},{"location":"api/metadata/#get_file_metadata","title":"get_file_metadata","text":"<pre><code>def get_file_metadata(\n    path: str | list[str],\n    format: str | None = None,\n    include_stats: bool = True,\n    include_schema: bool = True,\n    include_null_counts: bool = True,\n    include_memory_usage: bool = True,\n    include_duplicates: bool = True,\n    include_sample: bool = True,\n    sample_size: int = 5,\n    verbose: bool = False,\n    **kwargs\n) -&gt; dict[str, Any]\n</code></pre> <p>Extract comprehensive metadata from file(s).</p>"},{"location":"api/metadata/#parameters_5","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to file(s). <code>format</code> <code>str \\| None</code> File format (csv, parquet, json, etc.). <code>include_stats</code> <code>bool</code> Include basic statistics (min, max, mean, etc.). Default: <code>True</code> <code>include_schema</code> <code>bool</code> Include schema information (data types). Default: <code>True</code> <code>include_null_counts</code> <code>bool</code> Include null value counts. Default: <code>True</code> <code>include_memory_usage</code> <code>bool</code> Include memory usage information. Default: <code>True</code> <code>include_duplicates</code> <code>bool</code> Include duplicate row count. Default: <code>True</code> <code>include_sample</code> <code>bool</code> Include sample data. Default: <code>True</code> <code>sample_size</code> <code>int</code> Number of rows to include in sample. Default: <code>5</code> <code>verbose</code> <code>bool</code> Enable verbose output. Default: <code>False</code> <code>**kwargs</code> <code>Any</code> Additional arguments passed to the appropriate loader."},{"location":"api/metadata/#returns_5","title":"Returns","text":"Type Description <code>dict[str, Any]</code> Dictionary containing metadata about the file(s)."},{"location":"api/metadata/#example_5","title":"Example","text":"<pre><code>from flowerpower_io.metadata import get_file_metadata\n\n# Extract metadata from a CSV file\nmetadata = get_file_metadata(\"data.csv\", format=\"csv\")\nprint(metadata)\n\n# Extract metadata from multiple Parquet files\nmetadata = get_file_metadata([\"data1.parquet\", \"data2.parquet\"], format=\"parquet\")\nprint(metadata)\n</code></pre>"},{"location":"api/metadata/#get_database_metadata","title":"get_database_metadata","text":"<pre><code>def get_database_metadata(\n    type_: str,\n    table_name: str,\n    path: str | None = None,\n    connection_string: str | None = None,\n    username: str | None = None,\n    password: str | None = None,\n    server: str | None = None,\n    port: str | int | None = None,\n    database: str | None = None,\n    include_stats: bool = True,\n    include_schema: bool = True,\n    include_null_counts: bool = True,\n    include_memory_usage: bool = True,\n    include_duplicates: bool = True,\n    include_sample: bool = True,\n    sample_size: int = 5,\n    verbose: bool = False,\n    **kwargs\n) -&gt; dict[str, Any]\n</code></pre> <p>Extract comprehensive metadata from a database table.</p>"},{"location":"api/metadata/#parameters_6","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type (sqlite, duckdb, postgres, mysql, mssql, oracle). <code>table_name</code> <code>str</code> Name of the table to analyze. <code>path</code> <code>str \\| None</code> File path for SQLite or DuckDB databases. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>include_stats</code> <code>bool</code> Include basic statistics (min, max, mean, etc.). Default: <code>True</code> <code>include_schema</code> <code>bool</code> Include schema information (data types). Default: <code>True</code> <code>include_null_counts</code> <code>bool</code> Include null value counts. Default: <code>True</code> <code>include_memory_usage</code> <code>bool</code> Include memory usage information. Default: <code>True</code> <code>include_duplicates</code> <code>bool</code> Include duplicate row count. Default: <code>True</code> <code>include_sample</code> <code>bool</code> Include sample data. Default: <code>True</code> <code>sample_size</code> <code>int</code> Number of rows to include in sample. Default: <code>5</code> <code>verbose</code> <code>bool</code> Enable verbose output. Default: <code>False</code> <code>**kwargs</code> <code>Any</code> Additional arguments passed to the appropriate loader."},{"location":"api/metadata/#returns_6","title":"Returns","text":"Type Description <code>dict[str, Any]</code> Dictionary containing metadata about the database table."},{"location":"api/metadata/#example_6","title":"Example","text":"<pre><code>from flowerpower_io.metadata import get_database_metadata\n\n# Extract metadata from a SQLite table\nmetadata = get_database_metadata(\n    type_=\"sqlite\",\n    table_name=\"users\",\n    path=\"database.db\"\n)\nprint(metadata)\n\n# Extract metadata from a PostgreSQL table\nmetadata = get_database_metadata(\n    type_=\"postgres\",\n    table_name=\"users\",\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n)\nprint(metadata)\n</code></pre>"},{"location":"api/metadata/#get_metadata","title":"get_metadata","text":"<pre><code>def get_metadata(\n    source: str | pl.DataFrame | pl.LazyFrame | pd.DataFrame | pa.Table | pds.Dataset | duckdb.DuckDBPyRelation | datafusion.DataFrame,\n    source_type: str | None = None,\n    include_stats: bool = True,\n    include_schema: bool = True,\n    include_null_counts: bool = True,\n    include_memory_usage: bool = True,\n    include_duplicates: bool = True,\n    include_sample: bool = True,\n    sample_size: int = 5,\n    verbose: bool = False,\n    **kwargs\n) -&gt; dict[str, Any]\n</code></pre> <p>Generic function to extract metadata from various data sources.</p>"},{"location":"api/metadata/#parameters_7","title":"Parameters","text":"Parameter Type Description <code>source</code> <code>str \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| pa.Table \\| pds.Dataset \\| duckdb.DuckDBPyRelation \\| datafusion.DataFrame</code> Data source to analyze. <code>source_type</code> <code>str \\| None</code> Type of data source (file, dataframe, table, dataset, relation). <code>include_stats</code> <code>bool</code> Include basic statistics (min, max, mean, etc.). Default: <code>True</code> <code>include_schema</code> <code>bool</code> Include schema information (data types). Default: <code>True</code> <code>include_null_counts</code> <code>bool</code> Include null value counts. Default: <code>True</code> <code>include_memory_usage</code> <code>bool</code> Include memory usage information. Default: <code>True</code> <code>include_duplicates</code> <code>bool</code> Include duplicate row count. Default: <code>True</code> <code>include_sample</code> <code>bool</code> Include sample data. Default: <code>True</code> <code>sample_size</code> <code>int</code> Number of rows to include in sample. Default: <code>5</code> <code>verbose</code> <code>bool</code> Enable verbose output. Default: <code>False</code> <code>**kwargs</code> <code>Any</code> Additional arguments passed to the appropriate loader."},{"location":"api/metadata/#returns_7","title":"Returns","text":"Type Description <code>dict[str, Any]</code> Dictionary containing metadata about the data source."},{"location":"api/metadata/#example_7","title":"Example","text":"<p>```python from flowerpower_io.metadata import get_metadata</p>"},{"location":"api/metadata/#extract-metadata-from-a-dataframe","title":"Extract metadata from a DataFrame","text":"<p>df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [\"x\", \"y\", \"z\"]}) metadata = get_metadata(df) print(metadata)</p>"},{"location":"api/metadata/#extract-metadata-from-a-file","title":"Extract metadata from a file","text":"<p>metadata = get_metadata(\"data.csv\", source_type=\"file\") print(metadata)</p>"},{"location":"api/metadata/#extract-metadata-from-a-database-table","title":"Extract metadata from a database table","text":"<p>metadata = get_metadata(\"users\", source_type=\"database\", type_=\"sqlite\", path=\"database.db\") print(metadata)</p>"},{"location":"api/saver/","title":"Saver Classes","text":"<p>The <code>flowerpower-io</code> library provides several saver classes for writing data to various sources and formats. These classes extend the base classes and provide specialized functionality for specific data destinations.</p>"},{"location":"api/saver/#csv-saver","title":"CSV Saver","text":""},{"location":"api/saver/#csvsaver","title":"CSVSaver","text":"<pre><code>class CSVSaver(BaseFileWriter, gc=False)\n</code></pre> <p>Saver for CSV files.</p>"},{"location":"api/saver/#parameters","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to CSV file(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"csv\" <code>basename</code> <code>str \\| None</code> Basename for the output file(s). <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>mode</code> <code>str</code> Write mode (append, overwrite, delete_matching, error_if_exists). Default: \"append\" <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>csv_options</code> <code>dict[str, Any]</code> Additional CSV writer options."},{"location":"api/saver/#methods","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], basename: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None, mode: str \\| None = None, **kwargs) -&gt; dict[str, Any]</code> Write data to file. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example","title":"Example","text":"<pre><code>from flowerpower_io.saver import CSVSaver\n\n# Initialize CSV saver\nsaver = CSVSaver(\n    path=\"output/\",\n    basename=\"results\",\n    mode=\"append\"\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#parquet-saver","title":"Parquet Saver","text":""},{"location":"api/saver/#parquetsaver","title":"ParquetSaver","text":"<pre><code>class ParquetSaver(BaseDatasetWriter, gc=False)\n</code></pre> <p>Saver for Parquet files.</p>"},{"location":"api/saver/#parameters_1","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to Parquet file(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"parquet\" <code>basename</code> <code>str \\| None</code> Basename for the output file(s). <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>mode</code> <code>str</code> Write mode (append, overwrite, delete_matching, error_if_exists). Default: \"append\" <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>partition_by</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>partitioning_flavor</code> <code>str \\| None</code> Partitioning flavor for the dataset. <code>compression</code> <code>str</code> Compression codec for the dataset. Default: \"zstd\" <code>row_group_size</code> <code>int \\| None</code> Row group size for the dataset. Default: 250000 <code>max_rows_per_file</code> <code>int \\| None</code> Maximum number of rows per file. Default: 2500000 <code>is_pydala_dataset</code> <code>bool</code> Write data as a Pydala ParquetDataset. Default: <code>False</code> <code>parquet_options</code> <code>dict[str, Any]</code> Additional Parquet writer options."},{"location":"api/saver/#methods_1","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None, mode: str \\| None = None, delta_subset: str \\| None = None, alter_schema: bool = False, update_metadata: bool = True, timestamp_column: str \\| None = None, verbose: bool = False, **kwargs) -&gt; dict[str, Any]</code> Write data to dataset. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_1","title":"Example","text":"<pre><code>from flowerpower_io.saver import ParquetSaver\n\n# Initialize Parquet saver\nsaver = ParquetSaver(\n    path=\"output/\",\n    basename=\"results\",\n    mode=\"append\",\n    partition_by=\"category\",\n    compression=\"zstd\"\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#json-saver","title":"JSON Saver","text":""},{"location":"api/saver/#jsonsaver","title":"JSONSaver","text":"<pre><code>class JSONSaver(BaseFileWriter, gc=False)\n</code></pre> <p>Saver for JSON files.</p>"},{"location":"api/saver/#parameters_2","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to JSON file(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"json\" <code>basename</code> <code>str \\| None</code> Basename for the output file(s). <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>mode</code> <code>str</code> Write mode (append, overwrite, delete_matching, error_if_exists). Default: \"append\" <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>json_options</code> <code>dict[str, Any]</code> Additional JSON writer options."},{"location":"api/saver/#methods_2","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], basename: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None, mode: str \\| None = None, **kwargs) -&gt; dict[str, Any]</code> Write data to file. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_2","title":"Example","text":"<pre><code>from flowerpower_io.saver import JSONSaver\n\n# Initialize JSON saver\nsaver = JSONSaver(\n    path=\"output/\",\n    basename=\"results\",\n    mode=\"append\"\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#deltatable-saver","title":"DeltaTable Saver","text":""},{"location":"api/saver/#deltatablesaver","title":"DeltaTableSaver","text":"<pre><code>class DeltaTableSaver(BaseDatasetWriter, gc=False)\n</code></pre> <p>Saver for Delta Lake tables.</p>"},{"location":"api/saver/#parameters_3","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to Delta Lake table(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"deltatable\" <code>basename</code> <code>str \\| None</code> Basename for the output file(s). <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>mode</code> <code>str</code> Write mode (append, overwrite, delete_matching, error_if_exists). Default: \"append\" <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>partition_by</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>partitioning_flavor</code> <code>str \\| None</code> Partitioning flavor for the dataset. <code>compression</code> <code>str</code> Compression codec for the dataset. Default: \"zstd\" <code>row_group_size</code> <code>int \\| None</code> Row group size for the dataset. Default: 250000 <code>max_rows_per_file</code> <code>int \\| None</code> Maximum number of rows per file. Default: 2500000 <code>is_pydala_dataset</code> <code>bool</code> Write data as a Pydala ParquetDataset. Default: <code>False</code> <code>delta_options</code> <code>dict[str, Any]</code> Additional Delta Lake writer options."},{"location":"api/saver/#methods_3","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None, mode: str \\| None = None, delta_subset: str \\| None = None, alter_schema: bool = False, update_metadata: bool = True, timestamp_column: str \\| None = None, verbose: bool = False, **kwargs) -&gt; dict[str, Any]</code> Write data to dataset. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_3","title":"Example","text":"<pre><code>from flowerpower_io.saver import DeltaTableSaver\n\n# Initialize DeltaTable saver\nsaver = DeltaTableSaver(\n    path=\"delta_table/\",\n    basename=\"results\",\n    mode=\"append\",\n    partition_by=\"category\",\n    compression=\"zstd\"\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#pydala-saver","title":"Pydala Saver","text":""},{"location":"api/saver/#pydalasaver","title":"PydalaSaver","text":"<pre><code>class PydalaSaver(BaseDatasetWriter, gc=False)\n</code></pre> <p>Saver for Pydala datasets.</p>"},{"location":"api/saver/#parameters_4","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> Path or list of paths to Pydala dataset(s). <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> Storage-specific options for accessing remote filesystems. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"pydala\" <code>basename</code> <code>str \\| None</code> Basename for the output file(s). <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>mode</code> <code>str</code> Write mode (append, overwrite, delete_matching, error_if_exists). Default: \"append\" <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>schema_</code> <code>pa.Schema \\| None</code> PyArrow schema for the dataset. <code>partition_by</code> <code>str \\| list[str] \\| pds.Partitioning \\| None</code> Dataset partitioning scheme. <code>partitioning_flavor</code> <code>str \\| None</code> Partitioning flavor for the dataset. <code>compression</code> <code>str</code> Compression codec for the dataset. Default: \"zstd\" <code>row_group_size</code> <code>int \\| None</code> Row group size for the dataset. Default: 250000 <code>max_rows_per_file</code> <code>int \\| None</code> Maximum number of rows per file. Default: 2500000 <code>is_pydala_dataset</code> <code>bool</code> Write data as a Pydala ParquetDataset. Default: <code>False</code> <code>pydala_options</code> <code>dict[str, Any]</code> Additional Pydala writer options."},{"location":"api/saver/#methods_4","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None, mode: str \\| None = None, delta_subset: str \\| None = None, alter_schema: bool = False, update_metadata: bool = True, timestamp_column: str \\| None = None, verbose: bool = False, **kwargs) -&gt; dict[str, Any]</code> Write data to dataset. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_4","title":"Example","text":"<pre><code>from flowerpower_io.saver import PydalaSaver\n\n# Initialize Pydala saver\nsaver = PydalaSaver(\n    path=\"pydala_dataset/\",\n    basename=\"results\",\n    mode=\"append\",\n    partition_by=\"category\",\n    compression=\"zstd\"\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#mqtt-saver","title":"MQTT Saver","text":""},{"location":"api/saver/#mqttsaver","title":"MQTTSaver","text":"<pre><code>class MQTTSaver(BaseFileWriter, gc=False)\n</code></pre> <p>Saver for MQTT messages.</p>"},{"location":"api/saver/#parameters_5","title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| list[str]</code> MQTT topic(s) to publish to. <code>storage_options</code> <code>StorageOptions \\| AwsStorageOptions \\| GcsStorageOptions \\| AzureStorageOptions \\| GitLabStorageOptions \\| GitHubStorageOptions \\| dict[str, Any] \\| None</code> MQTT connection options. <code>fs</code> <code>AbstractFileSystem \\| None</code> Filesystem instance for handling file operations. <code>format</code> <code>str \\| None</code> File format extension (without dot). Default: \"mqtt\" <code>basename</code> <code>str \\| None</code> Basename for the output file(s). <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>mode</code> <code>str</code> Write mode (append, overwrite, delete_matching, error_if_exists). Default: \"append\" <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>mqtt_options</code> <code>dict[str, Any]</code> Additional MQTT writer options."},{"location":"api/saver/#methods_5","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], basename: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None, mode: str \\| None = None, **kwargs) -&gt; dict[str, Any]</code> Write data to file. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_5","title":"Example","text":"<pre><code>from flowerpower_io.saver import MQTTSaver\n\n# Initialize MQTT saver\nsaver = MQTTSaver(\n    path=\"sensors/temperature\",\n    storage_options={\n        \"host\": \"mqtt.example.com\",\n        \"port\": 1883,\n        \"username\": \"user\",\n        \"password\": \"password\"\n    }\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#database-savers","title":"Database Savers","text":""},{"location":"api/saver/#sqlitesaver","title":"SQLiteSaver","text":"<pre><code>class SQLiteSaver(BaseDatabaseWriter, gc=False)\n</code></pre> <p>Saver for SQLite databases.</p>"},{"location":"api/saver/#parameters_6","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"sqlite\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite database. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>mode</code> <code>str</code> Write mode (append, replace, fail). Default: \"append\" <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>sqlite_options</code> <code>dict[str, Any]</code> Additional SQLite writer options."},{"location":"api/saver/#methods_6","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], mode: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None) -&gt; dict[str, Any]</code> Write data to database. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_6","title":"Example","text":"<pre><code>from flowerpower_io.saver import SQLiteSaver\n\n# Initialize SQLite saver\nsaver = SQLiteSaver(\n    type_=\"sqlite\",\n    table_name=\"users\",\n    path=\"database.db\",\n    mode=\"append\"\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#duckdbsaver","title":"DuckDBSaver","text":"<pre><code>class DuckDBSaver(BaseDatabaseWriter, gc=False)\n</code></pre> <p>Saver for DuckDB databases.</p>"},{"location":"api/saver/#parameters_7","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"duckdb\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for DuckDB database. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>mode</code> <code>str</code> Write mode (append, replace, fail). Default: \"append\" <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>duckdb_options</code> <code>dict[str, Any]</code> Additional DuckDB writer options."},{"location":"api/saver/#methods_7","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], mode: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None) -&gt; dict[str, Any]</code> Write data to database. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_7","title":"Example","text":"<pre><code>from flowerpower_io.saver import DuckDBSaver\n\n# Initialize DuckDB saver\nsaver = DuckDBSaver(\n    type_=\"duckdb\",\n    table_name=\"users\",\n    path=\"database.db\",\n    mode=\"append\"\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#postgresqlsaver","title":"PostgreSQLSaver","text":"<pre><code>class PostgreSQLSaver(BaseDatabaseWriter, gc=False)\n</code></pre> <p>Saver for PostgreSQL databases.</p>"},{"location":"api/saver/#parameters_8","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"postgres\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite or DuckDB databases. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>mode</code> <code>str</code> Write mode (append, replace, fail). Default: \"append\" <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>postgres_options</code> <code>dict[str, Any]</code> Additional PostgreSQL writer options."},{"location":"api/saver/#methods_8","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], mode: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None) -&gt; dict[str, Any]</code> Write data to database. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_8","title":"Example","text":"<pre><code>from flowerpower_io.saver import PostgreSQLSaver\n\n# Initialize PostgreSQL saver\nsaver = PostgreSQLSaver(\n    type_=\"postgres\",\n    table_name=\"users\",\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\",\n    mode=\"append\"\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#mysqlsaver","title":"MySQLSaver","text":"<pre><code>class MySQLSaver(BaseDatabaseWriter, gc=False)\n</code></pre> <p>Saver for MySQL databases.</p>"},{"location":"api/saver/#parameters_9","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"mysql\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite or DuckDB databases. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>mode</code> <code>str</code> Write mode (append, replace, fail). Default: \"append\" <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>mysql_options</code> <code>dict[str, Any]</code> Additional MySQL writer options."},{"location":"api/saver/#methods_9","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], mode: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None) -&gt; dict[str, Any]</code> Write data to database. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_9","title":"Example","text":"<pre><code>from flowerpower_io.saver import MySQLSaver\n\n# Initialize MySQL saver\nsaver = MySQLSaver(\n    type_=\"mysql\",\n    table_name=\"users\",\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\",\n    mode=\"append\"\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#mssqlsaver","title":"MSSQLSaver","text":"<pre><code>class MSSQLSaver(BaseDatabaseWriter, gc=False)\n</code></pre> <p>Saver for Microsoft SQL Server databases.</p>"},{"location":"api/saver/#parameters_10","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"mssql\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite or DuckDB databases. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>mode</code> <code>str</code> Write mode (append, replace, fail). Default: \"append\" <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>mssql_options</code> <code>dict[str, Any]</code> Additional MSSQL writer options."},{"location":"api/saver/#methods_10","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], mode: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None) -&gt; dict[str, Any]</code> Write data to database. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_10","title":"Example","text":"<pre><code>from flowerpower_io.saver import MSSQLSaver\n\n# Initialize MSSQL saver\nsaver = MSSQLSaver(\n    type_=\"mssql\",\n    table_name=\"users\",\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\",\n    mode=\"append\"\n)\n\n# Write data\nmetadata = saver.write(df)\n</code></pre>"},{"location":"api/saver/#oraclesaver","title":"OracleSaver","text":"<pre><code>class OracleSaver(BaseDatabaseWriter, gc=False)\n</code></pre> <p>Saver for Oracle databases.</p>"},{"location":"api/saver/#parameters_11","title":"Parameters","text":"Parameter Type Description <code>type_</code> <code>str</code> Database type. Default: \"oracle\" <code>table_name</code> <code>str</code> Table name in the database. Default: \"\" <code>path</code> <code>str \\| None</code> File path for SQLite or DuckDB databases. <code>connection_string</code> <code>str \\| None</code> Connection string for SQLAlchemy-based databases. <code>username</code> <code>str \\| None</code> Username for the database. <code>password</code> <code>str \\| None</code> Password for the database. <code>server</code> <code>str \\| None</code> Server address for the database. <code>port</code> <code>str \\| int \\| None</code> Port number for the database. <code>database</code> <code>str \\| None</code> Database name. <code>ssl</code> <code>bool</code> Enable SSL. Default: <code>False</code> <code>_metadata</code> <code>dict[str, Any]</code> Internal metadata storage. <code>_data</code> <code>pa.Table \\| pl.DataFrame \\| pl.LazyFrame \\| pd.DataFrame \\| dict \\| list[dict] \\| None</code> Internal data storage. <code>_ddb_con</code> <code>duckdb.DuckDBPyConnection \\| None</code> Internal DuckDB connection. <code>_dtf_ctx</code> <code>datafusion.SessionContext \\| None</code> Internal DataFusion context. <code>mode</code> <code>str</code> Write mode (append, replace, fail). Default: \"append\" <code>concat</code> <code>bool</code> Concatenate multiple files into a single DataFrame. Default: <code>False</code> <code>unique</code> <code>bool \\| list[str] \\| str</code> Unique columns for deduplication. Default: <code>False</code> <code>oracle_options</code> <code>dict[str, Any]</code> Additional Oracle writer options."},{"location":"api/saver/#methods_11","title":"Methods","text":"Method Signature Description <code>write</code> <code>(data: pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any] \\| list[pl.DataFrame \\| pl.LazyFrame \\| pa.Table \\| pa.RecordBatch \\| pa.RecordBatchReader \\| pd.DataFrame \\| dict[str, Any]]], mode: str \\| None = None, concat: bool \\| None = None, unique: bool \\| list[str] \\| str \\| None = None) -&gt; dict[str, Any]</code> Write data to database. <code>metadata</code> <code>property</code> Get metadata for the written data."},{"location":"api/saver/#example_11","title":"Example","text":"<p>```python from flowerpower_io.saver import OracleSaver</p>"},{"location":"api/saver/#initialize-oracle-saver","title":"Initialize Oracle saver","text":"<p>saver = OracleSaver(     type_=\"oracle\",     table_name=\"users\",     host=\"localhost\",     username=\"user\",     password=\"password\",     database=\"mydb\",     mode=\"append\" )</p>"},{"location":"api/saver/#write-data","title":"Write data","text":"<p>metadata = saver.write(df)</p>"}]}