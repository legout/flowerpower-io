{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to flowerpower-io Documentation!","text":"<p><code>flowerpower_io</code> is a robust and extensible Python library designed for efficient data input/output (I/O) operations. It provides a unified framework for interacting with various file formats and database systems, abstracting away the complexities of data handling. The library focuses on seamless data transfer to and from popular data structures like Pandas DataFrames, Polars Dataframes, and PyArrow Tables.</p>"},{"location":"#key-features","title":"Key Features:","text":"<ul> <li>Unified I/O: A consistent API for diverse data sources and destinations.</li> <li>Flexible Data Structures: Seamless integration with Pandas DataFrames, Polars Dataframes, and PyArrow Tables.</li> <li>Extensible Architecture: Easily add support for new file formats or database systems.</li> <li>Comprehensive Metadata Handling: Capture and manage crucial information about your data.</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to dive in? Head over to the Quickstart guide to begin using <code>flowerpower-io</code>!</p>"},{"location":"advanced/","title":"Advanced Usage","text":"<p>This section delves into advanced features of <code>flowerpower-io</code>, covering performance optimization, integration with external systems, and specific database integrations like DuckDB and DataFusion.</p>"},{"location":"advanced/#1-performance-optimization","title":"1. Performance Optimization","text":"<p><code>flowerpower-io</code> is designed for efficiency, but you can further optimize performance with these tips:</p> <ul> <li>Batch Processing: For large datasets, consider processing data in batches to manage memory usage and improve throughput. <code>flowerpower-io</code> loaders and savers often support internal batching mechanisms.</li> <li>Lazy Loading with Polars: When working with Polars, leverage LazyFrames (<code>to_polars(lazy=True)</code>) to defer computation until necessary, which can significantly reduce memory footprint and improve performance for complex transformations.</li> <li>Efficient File Formats: Utilize binary file formats like Parquet over text-based formats like CSV for better performance due to columnar storage and compression.</li> <li>Storage Options: Properly configure <code>storage_options</code> for remote storage (e.g., S3, GCS) to optimize network transfers and authentication.</li> </ul>"},{"location":"advanced/#2-integration-with-external-systems","title":"2. Integration with External Systems","text":"<p><code>flowerpower-io</code> can be integrated into larger data pipelines and workflows.</p>"},{"location":"advanced/#message-queues-eg-mqtt","title":"Message Queues (e.g., MQTT)","text":"<p>The <code>PayloadReader</code> can consume data from message queues, allowing <code>flowerpower-io</code> to act as a data ingress point for real-time data streams.</p> <pre><code>from flowerpower_io.loader import PayloadReader\nimport paho.mqtt.client as mqtt\nimport time\n\n# This is a simplified example. In a real scenario, you'd handle\n# MQTT client setup, connection, and message consumption more robustly.\n\n# class MockMQTTClient:\n#     def __init__(self):\n#         self.payload = None\n#     def subscribe(self, topic): pass\n#     def on_message(self, client, userdata, msg):\n#         self.payload = msg.payload.decode('utf-8')\n\n# # Simulate an MQTT client and message\n# mock_client = MockMQTTClient()\n# # Simulate a message being received\n# # In a real system, this would be triggered by an actual MQTT message\n# # mock_client.on_message(None, None, type('obj', (object,), {'payload': b'{\"sensor_id\": \"A1\", \"temperature\": 25.5}'})())\n\n# # payload_reader = PayloadReader(mqtt_client=mock_client)\n\n# # # In a real application, you would continuously check for new payloads\n# # # For this example, we'll just simulate one message\n# # time.sleep(1) # Give some time for the \"message\" to arrive\n# # if payload_reader.has_new_payload():\n# #     data = payload_reader.read_payload()\n# #     print(\"Received data from MQTT:\", data)\n</code></pre>"},{"location":"advanced/#cloud-storage-s3-gcs-azure-blob","title":"Cloud Storage (S3, GCS, Azure Blob)","text":"<p>Utilize <code>fsspec_utils</code> for seamless interaction with various cloud storage providers.</p> <pre><code># Example of reading from S3 (requires appropriate AWS credentials configured)\n# from flowerpower_io.loader import ParquetFileReader\n\n# s3_reader = ParquetFileReader(\n#     path=\"s3://your-bucket/path/to/data.parquet\",\n#     storage_options={\"key\": \"YOUR_ACCESS_KEY\", \"secret\": \"YOUR_SECRET_KEY\"}\n# )\n# df_s3 = s3_reader.to_pandas()\n# print(\"Data read from S3:\")\n# print(df_s3.head())\n</code></pre>"},{"location":"advanced/#3-sql-integration","title":"3. SQL Integration","text":"<p><code>flowerpower-io</code> provides robust integration with SQL databases, including advanced features for querying and data manipulation.</p>"},{"location":"advanced/#duckdb-integration","title":"DuckDB Integration","text":"<p>Leverage DuckDB for in-process analytical processing with SQL. <code>flowerpower-io</code> can read directly into or write from DuckDB.</p> <pre><code>import duckdb\nimport pandas as pd\nfrom flowerpower_io.loader import DuckDBReader\nfrom flowerpower_io.saver import DuckDBWriter\nimport os\n\n# Create a dummy DuckDB file for demonstration\ndb_path = \"my_duckdb.db\"\nconn = duckdb.connect(database=db_path, read_only=False)\nconn.execute(\"CREATE TABLE users (id INTEGER, name VARCHAR)\")\nconn.execute(\"INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob')\")\nconn.close()\n\n# Read from DuckDB\nduckdb_reader = DuckDBReader(path=db_path, table_name=\"users\")\ndf_users = duckdb_reader.to_pandas()\nprint(\"Users from DuckDB:\")\nprint(df_users)\n\n# Write to DuckDB\nnew_users = pd.DataFrame({\"id\": [3, 4], \"name\": [\"Charlie\", \"Diana\"]})\nduckdb_writer = DuckDBWriter(path=db_path, table_name=\"users\")\nduckdb_writer.write(data=new_users, if_exists=\"append\") # Append new users\n\n# Verify appended data\nconn = duckdb.connect(database=db_path, read_only=True)\nprint(\"\\nAll users after append:\")\nprint(conn.execute(\"SELECT * FROM users\").fetchdf())\nconn.close()\n\n# Clean up\nos.remove(db_path)\n</code></pre>"},{"location":"advanced/#datafusion-integration-via-pyarrow","title":"DataFusion Integration (via PyArrow)","text":"<p><code>flowerpower-io</code> can work with data that DataFusion can process, often through PyArrow Tables. This allows for powerful query planning and execution.</p> <pre><code># from flowerpower_io.loader import ParquetFileReader\n# from pyarrow import dataset as pa_dataset\n# from datafusion import SessionContext\n\n# # Assuming you have a Parquet file\n# # parquet_reader = ParquetFileReader(path=\"path/to/your/large_data.parquet\")\n# # arrow_table = parquet_reader.to_pyarrow_table()\n\n# # # Create a DataFusion context\n# # ctx = SessionContext()\n# # ctx.register_record_batches(\"my_table\", [arrow_table.to_batches()])\n\n# # # Execute a query using DataFusion\n# # result = ctx.sql(\"SELECT COUNT(*) FROM my_table\").collect()\n# # print(\"Count from DataFusion query:\", result)\n</code></pre>"},{"location":"advanced/#4-customizing-io-behavior","title":"4. Customizing I/O Behavior","text":"<ul> <li>Custom Schemas: When writing, you can often provide a schema to ensure data types are correctly interpreted by the target system.</li> <li>Error Handling: Implement robust error handling around <code>flowerpower-io</code> calls to manage file not found errors, permission issues, or data conversion problems.</li> <li>Logging: Integrate <code>flowerpower-io</code> operations with your application's logging framework to monitor data flows and troubleshoot issues.</li> </ul> <p>This section provides a glimpse into the advanced capabilities of <code>flowerpower-io</code>. By combining these features, you can build sophisticated and high-performance data processing solutions.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>The <code>flowerpower-io</code> library is designed with a modular and extensible architecture, allowing for flexible data input/output operations across various formats and systems. At its core, the library follows a clear class hierarchy, promoting code reusability and maintainability.</p>"},{"location":"architecture/#core-design-principles","title":"Core Design Principles","text":"<ul> <li>Modularity: Separation of concerns, with distinct modules for base functionalities, loaders, savers, and metadata handling.</li> <li>Extensibility: Easy to add support for new file formats, database systems, or storage backends.</li> <li>Abstraction: Hides the complexities of underlying I/O operations from the user, providing a unified and intuitive API.</li> <li>Data Structure Agnostic: Supports seamless data transfer to and from popular data structures like Pandas DataFrames, Polars Dataframes, and PyArrow Tables.</li> </ul>"},{"location":"architecture/#class-hierarchy-overview","title":"Class Hierarchy Overview","text":"<p>The library's core is built around a hierarchy of base classes defined in <code>src/flowerpower_io/base.py</code>, which are then extended by specialized classes in the <code>loader/</code> and <code>saver/</code> modules.</p>"},{"location":"architecture/#base-classes","title":"Base Classes","text":"<ul> <li><code>BaseFileIO</code>: Foundational class for all file-based I/O operations. Handles path resolution, filesystem initialization, and storage option management across various backends (local, S3, GCS, Azure, GitHub, GitLab).<ul> <li><code>BaseFileReader</code>: Extends <code>BaseFileIO</code> for reading data from files. Supports various output formats (Pandas, Polars, PyArrow, DuckDB, DataFusion) and features like batch processing.</li> <li><code>BaseDatasetReader</code>: Specializes <code>BaseFileReader</code> for handling partitioned datasets, integrating with PyArrow Dataset and Pydala Dataset.</li> <li><code>BaseFileWriter</code>: Defines logic for writing data to files, managing output basename, concatenation, uniqueness, and write modes.</li> <li><code>BaseDatasetWriter</code>: Specializes <code>BaseFileWriter</code> for writing data as datasets, supporting partitioning, compression, and fine-grained control.</li> </ul> </li> <li><code>BaseDatabaseIO</code>: Foundational class for all database I/O operations. Manages connection strings, credentials, and provides methods for connecting to various SQL and NoSQL databases.<ul> <li><code>BaseDatabaseReader</code>: Provides methods for reading data from relational and non-relational databases into various DataFrame formats (Polars, Pandas, PyArrow).</li> <li><code>BaseDatabaseWriter</code>: Defines logic for writing data to databases, supporting different write modes and data conversion.</li> </ul> </li> </ul>"},{"location":"architecture/#loaders-and-savers","title":"Loaders and Savers","text":"<p><code>flowerpower-io</code> supports a wide array of data formats and integrates with various storage solutions through its specialized loader and saver classes.</p>"},{"location":"architecture/#file-based-io","title":"File-based I/O","text":"Format Loader Class(es) Saver Class(es) CSV <code>CSVFileReader</code>, <code>CSVDatasetReader</code> <code>CSVFileWriter</code>, <code>CSVDatasetWriter</code> Parquet <code>ParquetFileReader</code>, <code>ParquetDatasetReader</code>, <code>PydalaDatasetReader</code> <code>ParquetFileWriter</code>, <code>ParquetDatasetWriter</code>, <code>PydalaDatasetWriter</code> JSON <code>JsonFileReader</code>, <code>JsonDatasetReader</code> <code>JsonFileWriter</code>, <code>JsonDatasetWriter</code> DeltaTable <code>DeltaTableReader</code> <code>DeltaTableWriter</code> MQTT <code>PayloadReader</code> (for consuming MQTT payloads) (N/A - Loader only) <p>Supported Storage Backends (via <code>fsspec_utils</code>): - Local filesystem - AWS S3 - Google Cloud Storage (GCS) - Azure Blob Storage - GitHub - GitLab</p>"},{"location":"architecture/#database-io","title":"Database I/O","text":"Database Reader Class(es) Writer Class(es) SQLite <code>SQLiteReader</code> <code>SQLiteWriter</code> DuckDB <code>DuckDBReader</code> <code>DuckDBWriter</code> PostgreSQL <code>PostgreSQLReader</code> <code>PostgreSQLWriter</code> MySQL <code>MySQLReader</code> <code>MySQLWriter</code> MSSQL <code>MSSQLReader</code> <code>MSSQLWriter</code> Oracle <code>OracleDBReader</code> <code>OracleDBWriter</code>"},{"location":"contributing/","title":"Contributing to flowerpower-io","text":"<p>We welcome contributions to <code>flowerpower-io</code>! Whether it's reporting a bug, suggesting a new feature, or submitting a pull request, your help is valuable.</p>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>If you encounter any bugs, unexpected behavior, or have feature requests, please open an issue on our GitHub Issues page.</p> <p>When reporting an issue, please include: - A clear and concise description of the problem. - Steps to reproduce the behavior. - Expected behavior. - Actual behavior. - Your operating system and Python version. - <code>flowerpower-io</code> version. - Any relevant code snippets or error messages.</p>"},{"location":"contributing/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<p>We encourage you to contribute code to <code>flowerpower-io</code>. To submit a pull request:</p> <ol> <li>Fork the repository: Start by forking the <code>flowerpower-io</code> repository on GitHub.</li> <li>Clone your fork: Clone your forked repository to your local machine:     <pre><code>git clone https://github.com/your-username/flowerpower-io.git\ncd flowerpower-io\n</code></pre></li> <li>Create a new branch: Create a new branch for your changes:     <pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b bugfix/your-bug-description\n</code></pre></li> <li>Set up your development environment:     It is recommended to use <code>uv</code> or <code>pixi</code> for dependency management.     <pre><code># Using uv\nuv venv\nuv pip install -e \".[all]\"\n</code></pre> <pre><code># Using pixi\npixi install\n</code></pre></li> <li>Make your changes: Implement your feature or bug fix. Ensure your code adheres to the project's coding style and includes appropriate tests.</li> <li>Test your changes: Run the test suite to ensure your changes haven't introduced any regressions:     <pre><code>uv run pytest\n# or if using pixi\npixi run pytest\n</code></pre></li> <li>Commit your changes: Write clear and concise commit messages.     <pre><code>git commit -m \"feat: Add new awesome feature\"\n# or\ngit commit -m \"fix: Resolve critical bug in CSV loader\"\n</code></pre></li> <li>Push your branch: Push your changes to your forked repository:     <pre><code>git push origin feature/your-feature-name\n</code></pre></li> <li>Create a Pull Request: Open a pull request from your forked repository to the <code>main</code> branch of the official <code>flowerpower-io</code> repository. Provide a detailed description of your changes.</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<p>For local development, ensure you have Python 3.8+ installed.</p> <ol> <li>Clone the repository:     <pre><code>git clone https://github.com/flowerpower-io/flowerpower-io.git\ncd flowerpower-io\n</code></pre></li> <li>Install dependencies:     <pre><code># Using uv (recommended)\nuv venv\nuv pip install -e \".[all]\"\n</code></pre> <pre><code># Using pixi\npixi install\n</code></pre></li> <li>Run tests:     ```bash     uv run pytest     # or     pixi run pytest</li> </ol>"},{"location":"examples/","title":"Examples","text":"<p>This section provides practical examples demonstrating various loading, saving, and conversion scenarios using <code>flowerpower-io</code>. These examples showcase the library's flexibility in handling different data formats and I/O operations.</p>"},{"location":"examples/#1-reading-and-writing-csv-files","title":"1. Reading and Writing CSV Files","text":"<p>This example demonstrates how to read data from a CSV file into a Pandas DataFrame and then write it back to another CSV file.</p> <pre><code>import pandas as pd\nfrom flowerpower_io.loader import CSVFileReader\nfrom flowerpower_io.saver import CSVFileWriter\nimport os\n\n# Create a dummy CSV file for demonstration\nsample_csv_content = \"\"\"id,name,value\n1,apple,10\n2,banana,20\n3,orange,15\n\"\"\"\nwith open(\"sample.csv\", \"w\") as f:\n    f.write(sample_csv_content)\n\n# Read from CSV\ncsv_reader = CSVFileReader(path=\"sample.csv\")\ndf_from_csv = csv_reader.to_pandas()\nprint(\"Data read from sample.csv:\")\nprint(df_from_csv)\n\n# Write to CSV\noutput_csv_path = \"output.csv\"\ncsv_writer = CSVFileWriter(path=output_csv_path)\ncsv_writer.write(data=df_from_csv)\nprint(f\"\\nData written to {output_csv_path}\")\n\n# Clean up\nos.remove(\"sample.csv\")\nos.remove(output_csv_path)\n</code></pre>"},{"location":"examples/#2-parquet-operations-with-data-conversion","title":"2. Parquet Operations with Data Conversion","text":"<p>This example illustrates reading a Parquet file into a Polars DataFrame and then writing a Polars DataFrame to a new Parquet file. It also shows conversion between data structures.</p> <pre><code>import polars as pl\nimport pyarrow as pa\nfrom flowerpower_io.loader import ParquetFileReader\nfrom flowerpower_io.saver import ParquetFileWriter\nimport os\n\n# Create a dummy Parquet file for demonstration\ndata_to_parquet = pl.DataFrame({\n    \"product\": [\"A\", \"B\", \"C\"],\n    \"price\": [1.99, 0.50, 2.75]\n})\ntemp_parquet_path = \"temp_products.parquet\"\ndata_to_parquet.write_parquet(temp_parquet_path)\n\n# Read from Parquet into Polars DataFrame\nparquet_reader = ParquetFileReader(path=temp_parquet_path)\ndf_polars = parquet_reader.to_polars()\nprint(\"Data read from temp_products.parquet (Polars DataFrame):\")\nprint(df_polars)\n\n# Convert Polars DataFrame to PyArrow Table\narrow_table = df_polars.to_arrow()\nprint(\"\\nConverted to PyArrow Table:\")\nprint(arrow_table)\n\n# Write PyArrow Table to a new Parquet file\noutput_parquet_path = \"new_products.parquet\"\nparquet_writer = ParquetFileWriter(path=output_parquet_path)\nparquet_writer.write(data=arrow_table)\nprint(f\"\\nPyArrow Table written to {output_parquet_path}\")\n\n# Clean up\nos.remove(temp_parquet_path)\nos.remove(output_parquet_path)\n</code></pre>"},{"location":"examples/#3-sqlite-database-interaction","title":"3. SQLite Database Interaction","text":"<p>Demonstrates writing a Pandas DataFrame to a SQLite database and then reading data back using a custom SQL query.</p> <pre><code>import pandas as pd\nfrom flowerpower_io.saver import SQLiteWriter\nfrom flowerpower_io.loader import SQLiteReader\nimport os\n\ndb_file = \"my_database.db\"\ntable_name = \"sales_data\"\n\n# Create a Pandas DataFrame\nsales_data = pd.DataFrame({\n    \"region\": [\"East\", \"West\", \"North\", \"South\"],\n    \"sales\": [1000, 1500, 1200, 900],\n    \"quarter\": [1, 1, 2, 2]\n})\n\n# Write to SQLite\nsqlite_writer = SQLiteWriter(path=db_file, table_name=table_name)\nsqlite_writer.write(data=sales_data, if_exists=\"replace\") # Overwrite if table exists\nprint(f\"Data written to SQLite database '{db_file}' in table '{table_name}'.\")\n\n# Read from SQLite with a custom query\nsqlite_reader = SQLiteReader(path=db_file)\nquery = f\"SELECT region, SUM(sales) as total_sales FROM {table_name} GROUP BY region ORDER BY total_sales DESC\"\ndf_query_result = sqlite_reader.to_pandas(query=query)\nprint(\"\\nTotal sales by region (from SQLite query):\")\nprint(df_query_result)\n\n# Clean up\nos.remove(db_file)\n</code></pre>"},{"location":"examples/#4-loading-json-data-with-metadata","title":"4. Loading JSON Data with Metadata","text":"<p>This example shows how to load data from a JSON file and retrieve associated metadata during the process.</p> <p>```python from flowerpower_io.loader import JsonFileReader import pandas as pd import os</p> <p>json_file = \"user_profiles.json\" profiles_content = ''' [   {\"user_id\": 1, \"username\": \"alpha\", \"status\": \"active\"},   {\"user_id\": 2, \"username\": \"beta\", \"status\": \"inactive\"} ] ''' with open(json_file, \"w\") as f:     f.write(profiles_content)</p> <p>json_reader = JsonFileReader(path=json_file) df_profiles, metadata = json_reader.to_pandas(metadata=True)</p> <p>print(\"User Profiles Data:\") print(df_profiles) print(\"\\nMetadata retrieved:\") print(metadata)</p>"},{"location":"examples/#clean-up","title":"Clean up","text":"<p>os.remove(json_file)</p>"},{"location":"installation/","title":"Installation","text":"<p><code>flowerpower-io</code> can be installed using <code>pip</code>, <code>uv</code>, or <code>pixi</code>.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher.</li> </ul>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":""},{"location":"installation/#using-pip","title":"Using pip","text":"<p>The simplest way to install <code>flowerpower-io</code> is using <code>pip</code>:</p> <pre><code>pip install flowerpower-io\n</code></pre> <p>To install with all optional dependencies (e.g., for specific database connectors or file formats), you can use:</p> <pre><code>pip install flowerpower-io[all]\n</code></pre> <p>Or for specific extras, e.g., for Parquet and DuckDB support:</p> <pre><code>pip install flowerpower-io[parquet,duckdb]\n</code></pre>"},{"location":"installation/#using-uv","title":"Using uv","text":"<p>If you use <code>uv</code> for dependency management, you can install <code>flowerpower-io</code> as follows:</p> <pre><code>uv pip install flowerpower-io\n</code></pre> <p>For extras:</p> <pre><code>uv pip install flowerpower-io[all]\n</code></pre>"},{"location":"installation/#using-pixi","title":"Using pixi","text":"<p>For <code>pixi</code> users, add <code>flowerpower-io</code> to your project's dependencies:</p> <pre><code>pixi add flowerpower-io\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing dependencies: If you encounter <code>ModuleNotFoundError</code>, ensure you have installed the necessary optional dependencies for the formats or databases you are trying to use.</li> <li>Python version: Verify that you are using Python 3.8 or a newer version.</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will help you get started with <code>flowerpower-io</code> by demonstrating a simple data loading and saving workflow. We'll cover loading data from a JSON file and saving it to a CSV file.</p>"},{"location":"quickstart/#1-prepare-your-data","title":"1. Prepare Your Data","text":"<p>First, let's create a sample JSON file that we'll use for loading. Save the following content to a file named <code>sample_data.json</code> in your project directory:</p> <pre><code>[\n  {\n    \"id\": 1,\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\"\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Bob\",\n    \"age\": 24,\n    \"city\": \"London\"\n  },\n  {\n    \"id\": 3,\n    \"name\": \"Charlie\",\n    \"age\": 35,\n    \"city\": \"Paris\"\n  }\n]\n</code></pre>"},{"location":"quickstart/#2-load-data-from-json","title":"2. Load Data from JSON","text":"<p>Now, let's use <code>JsonFileReader</code> to load this data into a Pandas DataFrame.</p> <pre><code>import pandas as pd\nfrom flowerpower_io.loader import JsonFileReader\nimport os\n\n# Define the path to your sample JSON file\njson_file_path = \"sample_data.json\"\n\n# Create a dummy JSON file for demonstration if it doesn't exist\n# In a real scenario, this file would already exist.\nif not os.path.exists(json_file_path):\n    with open(json_file_path, \"w\") as f:\n        f.write('''\n[\n  {\n    \"id\": 1,\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\"\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Bob\",\n    \"age\": 24,\n    \"city\": \"London\"\n  },\n  {\n    \"id\": 3,\n    \"name\": \"Charlie\",\n    \"age\": 35,\n    \"city\": \"Paris\"\n  }\n]\n''')\n\n# Initialize JsonFileReader\njson_reader = JsonFileReader(path=json_file_path)\n\n# Load data into a Pandas DataFrame\ndf = json_reader.to_pandas()\n\nprint(\"Data loaded from JSON:\")\nprint(df.head())\n</code></pre>"},{"location":"quickstart/#3-save-data-to-csv","title":"3. Save Data to CSV","text":"<p>Next, we'll use <code>CSVFileWriter</code> to save the loaded DataFrame to a new CSV file.</p> <pre><code>from flowerpower_io.saver import CSVFileWriter\n\n# Define the output CSV file path\noutput_csv_path = \"output_data.csv\"\n\n# Initialize CSVFileWriter\ncsv_writer = CSVFileWriter(path=output_csv_path)\n\n# Write the DataFrame to CSV\ncsv_writer.write(data=df)\n\nprint(f\"\\nData successfully saved to {output_csv_path}\")\n\n# Optional: Verify the content of the saved CSV\nwith open(output_csv_path, 'r') as f:\n    print(\"\\nContent of the saved CSV file:\")\n    print(f.read())\n\n# Clean up the dummy files (optional for your actual project)\nos.remove(json_file_path)\nos.remove(output_csv_path)\nprint(f\"\\nCleaned up {json_file_path} and {output_csv_path}\")\n</code></pre> <p>This quickstart demonstrated a basic workflow of loading data from JSON and saving it to CSV using <code>flowerpower-io</code>. You can explore more advanced features and other supported formats in the Examples and API Reference sections.</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the <code>flowerpower-io</code> API reference documentation. This section provides detailed information about all public classes, functions, and methods available in the library.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>The <code>flowerpower-io</code> library provides a unified interface for reading and writing data from various sources and formats. The API is organized into several modules:</p> <ul> <li>Base Classes - Core classes for file and database operations</li> <li>Metadata Functions - Functions for extracting metadata from data sources</li> <li>Loader Classes - Classes for reading data from various sources</li> <li>Saver Classes - Classes for writing data to various destinations</li> </ul>"},{"location":"api/#quick-navigation","title":"Quick Navigation","text":""},{"location":"api/#base-classes","title":"Base Classes","text":"<p>The base classes form the foundation of the library and provide common functionality for all I/O operations.</p> <ul> <li>BaseFileIO - Base class for file I/O operations</li> <li>BaseFileReader - Base class for file reading operations</li> <li>BaseDatasetReader - Base class for dataset reading operations</li> <li>BaseFileWriter - Base class for file writing operations</li> <li>BaseDatasetWriter - Base class for dataset writing operations</li> <li>BaseDatabaseIO - Base class for database operations</li> <li>BaseDatabaseReader - Base class for database reading operations</li> <li>BaseDatabaseWriter - Base class for database writing operations</li> </ul>"},{"location":"api/#metadata-functions","title":"Metadata Functions","text":"<p>Metadata functions help you understand the structure and properties of your data before processing it.</p> <ul> <li>get_dataframe_metadata - Extract metadata from DataFrames</li> <li>get_pyarrow_table_metadata - Extract metadata from PyArrow Tables</li> <li>get_pyarrow_dataset_metadata - Extract metadata from PyArrow Datasets</li> <li>get_duckdb_relation_metadata - Extract metadata from DuckDB relations</li> <li>get_datafusion_relation_metadata - Extract metadata from DataFusion relations</li> <li>get_file_metadata - Extract metadata from files</li> <li>get_database_metadata - Extract metadata from database tables</li> <li>get_metadata - Generic metadata extraction function</li> </ul>"},{"location":"api/#loader-classes","title":"Loader Classes","text":"<p>Loader classes provide specialized functionality for reading data from various sources.</p>"},{"location":"api/#file-loaders","title":"File Loaders","text":"<ul> <li>CSVLoader - Load data from CSV files</li> <li>ParquetLoader - Load data from Parquet files</li> <li>JSONLoader - Load data from JSON files</li> <li>DeltaTableLoader - Load data from Delta Lake tables</li> <li>PydalaLoader - Load data from Pydala datasets</li> <li>MQTTLoader - Load data from MQTT messages</li> </ul>"},{"location":"api/#database-loaders","title":"Database Loaders","text":"<ul> <li>SQLiteLoader - Load data from SQLite databases</li> <li>DuckDBLoader - Load data from DuckDB databases</li> <li>PostgreSQLLoader - Load data from PostgreSQL databases</li> <li>MySQLLoader - Load data from MySQL databases</li> <li>MSSQLLoader - Load data from Microsoft SQL Server databases</li> <li>OracleLoader - Load data from Oracle databases</li> </ul>"},{"location":"api/#saver-classes","title":"Saver Classes","text":"<p>Saver classes provide specialized functionality for writing data to various destinations.</p>"},{"location":"api/#file-savers","title":"File Savers","text":"<ul> <li>CSVSaver - Save data to CSV files</li> <li>ParquetSaver - Save data to Parquet files</li> <li>JSONSaver - Save data to JSON files</li> <li>DeltaTableSaver - Save data to Delta Lake tables</li> <li>PydalaSaver - Save data to Pydala datasets</li> <li>MQTTSaver - Save data to MQTT messages</li> </ul>"},{"location":"api/#database-savers","title":"Database Savers","text":"<ul> <li>SQLiteSaver - Save data to SQLite databases</li> <li>DuckDBSaver - Save data to DuckDB databases</li> <li>PostgreSQLSaver - Save data to PostgreSQL databases</li> <li>MySQLSaver - Save data to MySQL databases</li> <li>MSSQLSaver - Save data to Microsoft SQL Server databases</li> <li>OracleSaver - Save data to Oracle databases</li> </ul>"},{"location":"api/#usage-examples","title":"Usage Examples","text":""},{"location":"api/#basic-file-operations","title":"Basic File Operations","text":"<pre><code>from flowerpower_io import CSVLoader, ParquetSaver\n\n# Load data from CSV\nloader = CSVLoader(\"data.csv\")\ndf = loader.to_polars()\n\n# Save data to Parquet\nsaver = ParquetSaver(\"output/\")\nsaver.write(df)\n</code></pre>"},{"location":"api/#database-operations","title":"Database Operations","text":"<pre><code>from flowerpower_io import PostgreSQLLoader, SQLiteSaver\n\n# Load from PostgreSQL\nloader = PostgreSQLLoader(\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\",\n    table_name=\"users\"\n)\ndf = loader.to_polars()\n\n# Save to SQLite\nsaver = SQLiteSaver(\n    path=\"database.db\",\n    table_name=\"users\"\n)\nsaver.write(df)\n</code></pre>"},{"location":"api/#metadata-extraction","title":"Metadata Extraction","text":"<pre><code>from flowerpower_io.metadata import get_dataframe_metadata\n\n# Get metadata from DataFrame\nmetadata = get_dataframe_metadata(df)\nprint(metadata)\n</code></pre>"},{"location":"api/#common-patterns","title":"Common Patterns","text":""},{"location":"api/#reading-multiple-files","title":"Reading Multiple Files","text":"<pre><code>from flowerpower_io import ParquetLoader\n\n# Load multiple Parquet files\nloader = ParquetLoader(\"data/*.parquet\")\ndf = loader.to_polars()\n</code></pre>"},{"location":"api/#writing-with-partitioning","title":"Writing with Partitioning","text":"<pre><code>from flowerpower_io import ParquetSaver\n\n# Save with partitioning\nsaver = ParquetSaver(\n    path=\"output/\",\n    partition_by=\"category\",\n    compression=\"zstd\"\n)\nsaver.write(df)\n</code></pre>"},{"location":"api/#database-connection-management","title":"Database Connection Management","text":"<pre><code>from flowerpower_io import PostgreSQLLoader\n\n# Using context manager for connection\nwith PostgreSQLLoader(\n    host=\"localhost\",\n    username=\"user\",\n    password=\"password\",\n    database=\"mydb\"\n) as loader:\n    df = loader.to_polars()\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>The library provides comprehensive error handling for various scenarios:</p> <pre><code>from flowerpower_io import CSVLoader\n\ntry:\n    loader = CSVLoader(\"nonexistent.csv\")\n    df = loader.to_polars()\nexcept FileNotFoundError:\n    print(\"File not found\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"api/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use <code>opt_dtypes=True</code> for better memory efficiency</li> <li>Use <code>batch_size</code> for large datasets</li> <li>Use <code>concat=False</code> when working with multiple files separately</li> <li>Use appropriate compression for your data format</li> <li>Use partitioning for large datasets</li> </ol>"},{"location":"api/#see-also","title":"See Also","text":"<ul> <li>Installation Guide</li> <li>Quick Start Guide</li> <li>Advanced Usage</li> <li>Architecture Overview</li> </ul>"},{"location":"api/base/","title":"Base Classes","text":"<p>This section details the foundational abstract classes in <code>flowerpower-io</code> that serve as the building blocks for all I/O operations. These classes define common interfaces and functionalities, which are then specialized by concrete loader and saver implementations.</p> <p>All base classes are located in <code>src/flowerpower_io/base.py</code>.</p>"},{"location":"api/base/#basefileio","title":"<code>BaseFileIO</code>","text":"<p><code>BaseFileIO</code> is the foundational class for all file-based input/output operations. It provides core functionalities related to file path management, filesystem interactions, and handling storage options for various backends.</p>"},{"location":"api/base/#attributes","title":"Attributes","text":"<ul> <li><code>path</code>: The file path or a list of file paths. Can be a local path or a URI for remote storage.</li> <li><code>format</code>: The format of the file (e.g., \"csv\", \"parquet\", \"json\").</li> <li><code>storage_options</code>: A dictionary of options for the filesystem backend (e.g., AWS S3 credentials).</li> </ul>"},{"location":"api/base/#methods","title":"Methods","text":"<ul> <li><code>__post_init__(self)</code>: Initializes the filesystem based on the provided path and storage options.</li> <li><code>protocol(self)</code>: Property that returns the protocol of the file path (e.g., \"s3\", \"file\").</li> <li><code>_base_path(self)</code>: Property that returns the base path of the file(s).</li> <li><code>_path(self)</code>: Property that returns the resolved file path(s).</li> <li><code>_glob_path(self)</code>: Property that returns the glob-ready file path(s).</li> <li><code>_root_path(self)</code>: Property that returns the root path for the file(s).</li> <li><code>list_files(self)</code>: Lists files based on the configured path.</li> </ul>"},{"location":"api/base/#basefilereader","title":"<code>BaseFileReader</code>","text":"<p><code>BaseFileReader</code> extends <code>BaseFileIO</code> and provides fundamental methods for reading data from files. It supports conversion to various data structures like Pandas DataFrames, Polars DataFrames, and PyArrow Tables, and offers features like batch processing and metadata retrieval.</p>"},{"location":"api/base/#methods_1","title":"Methods","text":"<ul> <li><code>_load(self, query: str | None = None, reload: bool = False, **kwargs)</code>: Internal method to load data.</li> <li><code>to_pandas(self, metadata: bool = False, **kwargs)</code>: Reads data into a Pandas DataFrame.</li> <li><code>iter_pandas(self, **kwargs)</code>: Returns an iterator for Pandas DataFrames.</li> <li><code>to_polars(self, lazy: bool = False, metadata: bool = False, **kwargs)</code>: Reads data into a Polars DataFrame or LazyFrame.</li> <li><code>iter_polars(self, lazy: bool = False, **kwargs)</code>: Returns an iterator for Polars DataFrames or LazyFrames.</li> <li><code>to_pyarrow_table(self, metadata: bool = False, **kwargs)</code>: Reads data into a PyArrow Table.</li> <li><code>iter_pyarrow_table(self, **kwargs)</code>: Returns an iterator for PyArrow Tables.</li> <li><code>to_duckdb_relation(self, connection: duckdb.DuckDBPyConnection | None = None, **kwargs)</code>: Reads data into a DuckDB Relation.</li> <li><code>register_in_duckdb(self, connection: duckdb.DuckDBPyConnection, table_name: str, **kwargs)</code>: Registers the data as a table in a DuckDB connection.</li> <li><code>to_duckdb(self, query: str, connection: duckdb.DuckDBPyConnection | None = None, **kwargs)</code>: Executes a SQL query against the data using DuckDB.</li> <li><code>register_in_datafusion(self, context: SessionContext, table_name: str, **kwargs)</code>: Registers the data as a table in a DataFusion context.</li> <li><code>filter(self, filter_expr)</code>: Applies a filter expression to the data before loading.</li> <li><code>metadata(self)</code>: Property that returns metadata about the loaded data.</li> </ul>"},{"location":"api/base/#basedatasetreader","title":"<code>BaseDatasetReader</code>","text":"<p><code>BaseDatasetReader</code> extends <code>BaseFileReader</code> to handle dataset-specific reading operations, particularly for partitioned datasets. It provides specialized methods for working with PyArrow Datasets and Pydala Datasets.</p>"},{"location":"api/base/#methods_2","title":"Methods","text":"<ul> <li><code>to_pyarrow_dataset(self, **kwargs)</code>: Reads data into a PyArrow Dataset.</li> <li><code>to_pandas(self, metadata: bool = False, **kwargs)</code>: Overrides <code>BaseFileReader.to_pandas</code> for dataset-specific loading.</li> <li><code>to_polars(self, lazy: bool = False, metadata: bool = False, **kwargs)</code>: Overrides <code>BaseFileReader.to_polars</code> for dataset-specific loading.</li> <li><code>to_pyarrow_table(self, metadata: bool = False, **kwargs)</code>: Overrides <code>BaseFileReader.to_pyarrow_table</code> for dataset-specific loading.</li> <li><code>to_pydala_dataset(self, **kwargs)</code>: Reads data into a Pydala Dataset.</li> <li><code>to_duckdb_relation(self, connection: duckdb.DuckDBPyConnection | None = None, **kwargs)</code>: Overrides <code>BaseFileReader.to_duckdb_relation</code> for dataset-specific loading.</li> <li><code>register_in_duckdb(self, connection: duckdb.DuckDBPyConnection, table_name: str, **kwargs)</code>: Overrides <code>BaseFileReader.register_in_duckdb</code> for dataset-specific registration.</li> <li><code>to_duckdb(self, query: str, connection: duckdb.DuckDBPyConnection | None = None, **kwargs)</code>: Overrides <code>BaseFileReader.to_duckdb</code> for dataset-specific querying.</li> <li><code>register_in_datafusion(self, context: SessionContext, table_name: str, **kwargs)</code>: Overrides <code>BaseFileReader.register_in_datafusion</code> for dataset-specific registration.</li> <li><code>filter(self, filter_expr)</code>: Overrides <code>BaseFileReader.filter</code> for dataset-specific filtering.</li> <li><code>metadata(self)</code>: Property that returns metadata about the dataset.</li> </ul>"},{"location":"api/base/#basefilewriter","title":"<code>BaseFileWriter</code>","text":"<p><code>BaseFileWriter</code> defines the core logic for writing data to files. It manages output paths, uniqueness constraints, and various write modes.</p>"},{"location":"api/base/#methods_3","title":"Methods","text":"<ul> <li><code>write(self, data: pd.DataFrame | pl.DataFrame | pa.Table, **kwargs)</code>: Writes data to the specified file path.<ul> <li><code>data</code>: The data to write (Pandas DataFrame, Polars DataFrame, or PyArrow Table).</li> <li><code>**kwargs</code>: Additional keyword arguments for the specific file format writer.</li> </ul> </li> <li><code>metadata(self)</code>: Property that returns metadata about the write operation.</li> </ul>"},{"location":"api/base/#basedatasetwriter","title":"<code>BaseDatasetWriter</code>","text":"<p><code>BaseDatasetWriter</code> specializes <code>BaseFileWriter</code> for writing data as datasets, supporting partitioning, compression, and fine-grained control over file and row group sizes. It also integrates with Pydala for advanced dataset writing.</p>"},{"location":"api/base/#methods_4","title":"Methods","text":"<ul> <li><code>write(self, data: pd.DataFrame | pl.DataFrame | pa.Table, **kwargs)</code>: Writes data as a dataset.<ul> <li><code>data</code>: The data to write.</li> <li><code>**kwargs</code>: Additional keyword arguments for the specific dataset writer.</li> </ul> </li> <li><code>metadata(self)</code>: Property that returns metadata about the dataset write operation.</li> </ul>"},{"location":"api/base/#basedatabaseio","title":"<code>BaseDatabaseIO</code>","text":"<p><code>BaseDatabaseIO</code> is the foundational class for all database input/output operations. It manages database connection strings, credentials, and provides methods for connecting to various SQL and NoSQL databases.</p>"},{"location":"api/base/#attributes_1","title":"Attributes","text":"<ul> <li><code>connection_string</code>: The connection string for the database.</li> <li><code>server</code>: The database server address.</li> <li><code>port</code>: The database port.</li> <li><code>username</code>: The database username.</li> <li><code>password</code>: The database password.</li> <li><code>database</code>: The database name.</li> <li><code>table_name</code>: The name of the table to interact with.</li> <li><code>query</code>: A SQL query to execute.</li> <li><code>path</code>: File path for file-based databases (e.g., SQLite, DuckDB).</li> <li><code>type_</code>: The type of database (e.g., \"sqlite\", \"postgresql\").</li> </ul>"},{"location":"api/base/#methods_5","title":"Methods","text":"<ul> <li><code>__post_init__(self)</code>: Initializes the database connection.</li> <li><code>execute(self, query: str, cursor: bool = True, **query_kwargs)</code>: Executes a SQL query.</li> <li><code>_to_pandas(self, data: pl.DataFrame | pa.Table)</code>: Internal method to convert data to Pandas DataFrame.</li> <li><code>connect(self)</code>: Establishes a connection to the database.</li> </ul>"},{"location":"api/base/#basedatabasewriter","title":"<code>BaseDatabaseWriter</code>","text":"<p><code>BaseDatabaseWriter</code> defines the core logic for writing data to databases, supporting different write modes and handling data conversion for various database types.</p>"},{"location":"api/base/#methods_6","title":"Methods","text":"<ul> <li><code>_write_sqlite(self, data: pl.DataFrame)</code>: Internal method to write data to SQLite.</li> <li><code>_write_duckdb(self, data: pl.DataFrame)</code>: Internal method to write data to DuckDB.</li> <li><code>_write_sqlalchemy(self, data: pl.DataFrame)</code>: Internal method to write data using SQLAlchemy.</li> <li><code>write(self, data: pd.DataFrame | pl.DataFrame | pa.Table, **kwargs)</code>: Writes data to the specified database table.<ul> <li><code>data</code>: The data to write.</li> <li><code>**kwargs</code>: Additional keyword arguments for the specific database writer (e.g., <code>if_exists</code>).</li> </ul> </li> <li><code>metadata(self)</code>: Property that returns metadata about the database write operation.</li> </ul>"},{"location":"api/base/#basedatabasereader","title":"<code>BaseDatabaseReader</code>","text":"<p><code>BaseDatabaseReader</code> provides methods for reading data from relational and non-relational databases into various DataFrame formats (Polars, Pandas, PyArrow) and integrates with DuckDB and DataFusion for SQL query execution.</p>"},{"location":"api/base/#methods_7","title":"Methods","text":"<ul> <li><code>__post_init__(self)</code>: Initializes the database reader.</li> <li><code>_load(self, query: str | None = None, reload: bool = False, **kwargs)</code>: Internal method to load data from the database.</li> <li><code>to_polars(self, **kwargs)</code>: Reads data into a Polars DataFrame.</li> <li><code>to_pandas(self, **kwargs)</code>: Reads data into a Pandas DataFrame.</li> <li><code>to_pyarrow_table(self, **kwargs)</code>: Reads data into a PyArrow Table.</li> <li><code>to_duckdb_relation(self, connection: duckdb.DuckDBPyConnection | None = None, **kwargs)</code>: Reads data into a DuckDB Relation.</li> <li><code>register_in_duckdb(self, connection: duckdb.DuckDBPyConnection, table_name: str, **kwargs)</code>: Registers the data as a table in a DuckDB connection.</li> <li><code>register_in_datafusion(self, context: SessionContext, table_name: str, **kwargs)</code>: Registers the data as a table in a DataFusion context.</li> <li><code>metadata(self)</code>: Property that returns metadata about the database read operation.</li> </ul>"},{"location":"api/loader/","title":"Loaders","text":"<p>The <code>flowerpower-io</code> library provides a comprehensive set of loader classes, each specialized for reading data from specific file formats or database systems. These loaders extend <code>BaseFileReader</code> or <code>BaseDatabaseReader</code> and offer tailored functionalities for efficient data ingestion.</p>"},{"location":"api/loader/#file-based-loaders","title":"File-based Loaders","text":""},{"location":"api/loader/#csvfilereader","title":"<code>CSVFileReader</code>","text":"<p>Reads data from CSV files. - Path: <code>src/flowerpower_io/loader/csv.py</code> - Example:   <pre><code>from flowerpower_io.loader import CSVFileReader\nimport pandas as pd\n\ncsv_reader = CSVFileReader(path=\"data.csv\")\ndf = csv_reader.to_pandas()\n</code></pre></p>"},{"location":"api/loader/#parquetfilereader","title":"<code>ParquetFileReader</code>","text":"<p>Reads data from Parquet files. - Path: <code>src/flowerpower_io/loader/parquet.py</code> - Example:   <pre><code>from flowerpower_io.loader import ParquetFileReader\nimport polars as pl\n\nparquet_reader = ParquetFileReader(path=\"data.parquet\")\ndf = parquet_reader.to_polars()\n</code></pre></p>"},{"location":"api/loader/#jsonfilereader","title":"<code>JsonFileReader</code>","text":"<p>Reads data from JSON files. - Path: <code>src/flowerpower_io/loader/json.py</code> - Example:   <pre><code>from flowerpower_io.loader import JsonFileReader\nimport pyarrow as pa\n\njson_reader = JsonFileReader(path=\"data.json\")\ntable = json_reader.to_pyarrow_table()\n</code></pre></p>"},{"location":"api/loader/#deltatablereader","title":"<code>DeltaTableReader</code>","text":"<p>Reads data from Delta Lake tables. - Path: <code>src/flowerpower_io/loader/deltatable.py</code> - Example:   <pre><code>from flowerpower_io.loader import DeltaTableReader\nimport pandas as pd\n\ndelta_reader = DeltaTableReader(path=\"path/to/delta_table\")\ndf = delta_reader.to_pandas()\n</code></pre></p>"},{"location":"api/loader/#payloadreader-mqtt","title":"<code>PayloadReader</code> (MQTT)","text":"<p>Reads payloads from MQTT messages. This is a specialized loader for streaming data. - Path: <code>src/flowerpower_io/loader/mqtt.py</code> - Example:   <pre><code># This example is conceptual, as MQTT integration requires a running broker and client setup.\n# from flowerpower_io.loader import PayloadReader\n# from paho.mqtt.client import Client as MQTTClient\n\n# mqtt_client = MQTTClient()\n# # ... configure and connect mqtt_client ...\n# payload_reader = PayloadReader(mqtt_client=mqtt_client)\n# payload = payload_reader.read_payload()\n# print(payload)\n</code></pre></p>"},{"location":"api/loader/#database-loaders","title":"Database Loaders","text":""},{"location":"api/loader/#sqlitereader","title":"<code>SQLiteReader</code>","text":"<p>Reads data from SQLite databases. - Path: <code>src/flowerpower_io/loader/sqlite.py</code> - Example:   <pre><code>from flowerpower_io.loader import SQLiteReader\nimport pandas as pd\n\nsqlite_reader = SQLiteReader(path=\"my_database.db\", table_name=\"my_table\")\ndf = sqlite_reader.to_pandas()\n</code></pre></p>"},{"location":"api/loader/#duckdbreader","title":"<code>DuckDBReader</code>","text":"<p>Reads data from DuckDB databases. - Path: <code>src/flowerpower_io/loader/duckdb.py</code> - Example:   <pre><code>from flowerpower_io.loader import DuckDBReader\nimport polars as pl\n\nduckdb_reader = DuckDBReader(path=\"my_duckdb.db\", table_name=\"another_table\")\ndf = duckdb_reader.to_polars()\n</code></pre></p>"},{"location":"api/loader/#postgresqlreader","title":"<code>PostgreSQLReader</code>","text":"<p>Reads data from PostgreSQL databases. - Path: <code>src/flowerpower_io/loader/postgres.py</code> - Example:   <pre><code>from flowerpower_io.loader import PostgreSQLReader\nimport pandas as pd\n\npg_reader = PostgreSQLReader(\n    database=\"mydb\",\n    user=\"myuser\",\n    password=\"mypassword\",\n    host=\"localhost\",\n    table_name=\"users\"\n)\ndf = pg_reader.to_pandas()\n</code></pre></p>"},{"location":"api/loader/#mysqlreader","title":"<code>MySQLReader</code>","text":"<p>Reads data from MySQL databases. - Path: <code>src/flowerpower_io/loader/mysql.py</code> - Example:   <pre><code>from flowerpower_io.loader import MySQLReader\nimport pandas as pd\n\nmysql_reader = MySQLReader(\n    database=\"mydb\",\n    user=\"myuser\",\n    password=\"mypassword\",\n    host=\"localhost\",\n    table_name=\"products\"\n)\ndf = mysql_reader.to_pandas()\n</code></pre></p>"},{"location":"api/loader/#mssqlreader","title":"<code>MSSQLReader</code>","text":"<p>Reads data from Microsoft SQL Server databases. - Path: <code>src/flowerpower_io/loader/mssql.py</code> - Example:   <pre><code>from flowerpower_io.loader import MSSQLReader\nimport pandas as pd\n\nmssql_reader = MSSQLReader(\n    database=\"mydb\",\n    user=\"myuser\",\n    password=\"mypassword\",\n    host=\"localhost\",\n    table_name=\"orders\"\n)\ndf = mssql_reader.to_pandas()\n</code></pre></p>"},{"location":"api/loader/#oracledbreader","title":"<code>OracleDBReader</code>","text":"<p>Reads data from Oracle databases. - Path: <code>src/flowerpower_io/loader/oracle.py</code> - Example:   ```python   from flowerpower_io.loader import OracleDBReader   import pandas as pd</p> <p>oracle_reader = OracleDBReader(       database=\"mydb\",       user=\"myuser\",       password=\"mypassword\",       host=\"localhost\",       table_name=\"inventory\"   )   df = oracle_reader.to_pandas()</p>"},{"location":"api/metadata/","title":"Metadata","text":"<p>The <code>src/flowerpower_io/metadata.py</code> module is crucial for collecting and managing detailed metadata during I/O operations. This includes information about data schema, row and column counts, file paths, and timestamps of operations. This metadata can be invaluable for data governance, auditing, and understanding data lineage.</p>"},{"location":"api/metadata/#functions","title":"Functions","text":""},{"location":"api/metadata/#get_serializable_schemadata-any-dict","title":"<code>get_serializable_schema(data: Any) -&gt; dict</code>","text":"<p>Generates a JSON-serializable schema representation of the input data. This function inspects the data (e.g., Pandas DataFrame, Polars DataFrame, PyArrow Table) and extracts its schema, converting it into a dictionary format suitable for serialization.</p> <ul> <li>Parameters:<ul> <li><code>data</code>: The input data (Pandas DataFrame, Polars DataFrame, PyArrow Table, etc.).</li> </ul> </li> <li>Returns:<ul> <li><code>dict</code>: A dictionary representing the serializable schema of the data.</li> </ul> </li> </ul>"},{"location":"api/metadata/#get_dataframe_metadatadf-pddataframe-pldataframe-patable-dict","title":"<code>get_dataframe_metadata(df: pd.DataFrame | pl.DataFrame | pa.Table) -&gt; dict</code>","text":"<p>Extracts comprehensive metadata from a given DataFrame or PyArrow Table. This includes schema information, row count, column count, and a timestamp of when the metadata was generated.</p> <ul> <li>Parameters:<ul> <li><code>df</code>: The input DataFrame (Pandas DataFrame, Polars DataFrame, or PyArrow Table).</li> </ul> </li> <li>Returns:<ul> <li><code>dict</code>: A dictionary containing the extracted metadata.</li> </ul> </li> </ul>"},{"location":"api/metadata/#get_duckdb_metadataconnection-duckdbduckdbpyconnection-table_name-str-dict","title":"<code>get_duckdb_metadata(connection: duckdb.DuckDBPyConnection, table_name: str) -&gt; dict</code>","text":"<p>Retrieves metadata specifically from a DuckDB table. This function connects to a DuckDB database and extracts schema and other relevant information for the specified table.</p> <ul> <li>Parameters:<ul> <li><code>connection</code>: An active DuckDB connection object.</li> <li><code>table_name</code>: The name of the table in the DuckDB database.</li> </ul> </li> <li>Returns:<ul> <li><code>dict</code>: A dictionary containing metadata about the DuckDB table.</li> </ul> </li> </ul>"},{"location":"api/metadata/#get_pyarrow_dataset_metadatadataset-padatasetdataset-dict","title":"<code>get_pyarrow_dataset_metadata(dataset: pa.dataset.Dataset) -&gt; dict</code>","text":"<p>Extracts metadata from a PyArrow Dataset. This includes information about the dataset's schema, partitioning, and the files it comprises.</p> <ul> <li>Parameters:<ul> <li><code>dataset</code>: A PyArrow Dataset object.</li> </ul> </li> <li>Returns:<ul> <li><code>dict</code>: A dictionary containing metadata about the PyArrow Dataset.</li> </ul> </li> </ul>"},{"location":"api/metadata/#get_delta_metadatapath-str-dict","title":"<code>get_delta_metadata(path: str) -&gt; dict</code>","text":"<p>Retrieves metadata from a Delta Lake table located at the specified path. This function provides insights into the Delta table's structure, versioning, and other properties.</p> <ul> <li>Parameters:<ul> <li><code>path</code>: The file path to the Delta Lake table.</li> </ul> </li> <li>Returns:<ul> <li><code>dict</code>: A dictionary containing metadata about the Delta Lake table.</li> </ul> </li> </ul>"},{"location":"api/metadata/#get_mqtt_metadatapayload-bytes-dict","title":"<code>get_mqtt_metadata(payload: bytes) -&gt; dict</code>","text":"<p>(Conceptual) Extracts metadata from an MQTT message payload. This function is designed for real-time data streams, parsing the payload to extract relevant metadata.</p> <ul> <li>Parameters:<ul> <li><code>payload</code>: The raw byte payload from an MQTT message.</li> </ul> </li> <li>Returns:<ul> <li><code>dict</code>: A dictionary containing metadata extracted from the MQTT payload.</li> </ul> </li> </ul>"},{"location":"api/saver/","title":"Savers","text":"<p>The <code>flowerpower-io</code> library provides a comprehensive set of saver classes, each specialized for writing data to specific file formats or database systems. These savers extend <code>BaseFileWriter</code> or <code>BaseDatabaseWriter</code> and offer tailored functionalities for efficient data persistence.</p>"},{"location":"api/saver/#file-based-savers","title":"File-based Savers","text":""},{"location":"api/saver/#csvfilewriter","title":"<code>CSVFileWriter</code>","text":"<p>Writes data to CSV files. - Path: <code>src/flowerpower_io/saver/csv.py</code> - Example:   <pre><code>from flowerpower_io.saver import CSVFileWriter\nimport pandas as pd\n\ndf = pd.DataFrame({'col1': [1, 2], 'col2': ['A', 'B']})\ncsv_writer = CSVFileWriter(path=\"output.csv\")\ncsv_writer.write(data=df)\n</code></pre></p>"},{"location":"api/saver/#parquetfilewriter","title":"<code>ParquetFileWriter</code>","text":"<p>Writes data to Parquet files. - Path: <code>src/flowerpower_io/saver/parquet.py</code> - Example:   <pre><code>from flowerpower_io.saver import ParquetFileWriter\nimport polars as pl\n\ndf = pl.DataFrame({'col1': [1, 2], 'col2': ['A', 'B']})\nparquet_writer = ParquetFileWriter(path=\"output.parquet\")\nparquet_writer.write(data=df)\n</code></pre></p>"},{"location":"api/saver/#jsonfilewriter","title":"<code>JsonFileWriter</code>","text":"<p>Writes data to JSON files. - Path: <code>src/flowerpower_io/saver/json.py</code> - Example:   <pre><code>from flowerpower_io.saver import JsonFileWriter\nimport pyarrow as pa\n\ntable = pa.table({'col1': [1, 2], 'col2': ['A', 'B']})\njson_writer = JsonFileWriter(path=\"output.json\")\njson_writer.write(data=table)\n</code></pre></p>"},{"location":"api/saver/#deltatablewriter","title":"<code>DeltaTableWriter</code>","text":"<p>Writes data to Delta Lake tables. - Path: <code>src/flowerpower_io/saver/deltatable.py</code> - Example:   <pre><code>from flowerpower_io.saver import DeltaTableWriter\nimport pandas as pd\n\ndf = pd.DataFrame({'col1': [1, 2], 'col2': ['A', 'B']})\ndelta_writer = DeltaTableWriter(path=\"path/to/delta_table\")\ndelta_writer.write(data=df)\n</code></pre></p>"},{"location":"api/saver/#mqttwriter-conceptual","title":"<code>MQTTWriter</code> (Conceptual)","text":"<p>Writes data as MQTT messages. - Path: <code>src/flowerpower_io/saver/mqtt.py</code> - Example:   <pre><code># This example is conceptual, as MQTT integration requires a running broker and client setup.\n# from flowerpower_io.saver import MQTTWriter\n# import pandas as pd\n\n# df = pd.DataFrame({'sensor_id': ['A1'], 'value': [10.5]})\n# mqtt_writer = MQTTWriter(topic=\"sensor/data\", host=\"localhost\")\n# mqtt_writer.write(data=df)\n</code></pre></p>"},{"location":"api/saver/#database-savers","title":"Database Savers","text":""},{"location":"api/saver/#sqlitewriter","title":"<code>SQLiteWriter</code>","text":"<p>Writes data to SQLite databases. - Path: <code>src/flowerpower_io/saver/sqlite.py</code> - Example:   <pre><code>from flowerpower_io.saver import SQLiteWriter\nimport pandas as pd\n\ndf = pd.DataFrame({'id': [1, 2], 'name': ['Alice', 'Bob']})\nsqlite_writer = SQLiteWriter(path=\"my_database.db\", table_name=\"users\")\nsqlite_writer.write(data=df)\n</code></pre></p>"},{"location":"api/saver/#duckdbwriter","title":"<code>DuckDBWriter</code>","text":"<p>Writes data to DuckDB databases. - Path: <code>src/flowerpower_io/saver/duckdb.py</code> - Example:   <pre><code>from flowerpower_io.saver import DuckDBWriter\nimport polars as pl\n\ndf = pl.DataFrame({'id': [1, 2], 'product': ['Apple', 'Banana']})\nduckdb_writer = DuckDBWriter(path=\"my_duckdb.db\", table_name=\"products\")\nduckdb_writer.write(data=df)\n</code></pre></p>"},{"location":"api/saver/#postgresqlwriter","title":"<code>PostgreSQLWriter</code>","text":"<p>Writes data to PostgreSQL databases. - Path: <code>src/flowerpower_io/saver/postgres.py</code> - Example:   <pre><code>from flowerpower_io.saver import PostgreSQLWriter\nimport pandas as pd\n\ndf = pd.DataFrame({'id': [1, 2], 'event': ['login', 'logout']})\npg_writer = PostgreSQLWriter(\n    database=\"mydb\",\n    user=\"myuser\",\n    password=\"mypassword\",\n    host=\"localhost\",\n    table_name=\"events\"\n)\npg_writer.write(data=df)\n</code></pre></p>"},{"location":"api/saver/#mysqlwriter","title":"<code>MySQLWriter</code>","text":"<p>Writes data to MySQL databases. - Path: <code>src/flowerpower_io/saver/mysql.py</code> - Example:   <pre><code>from flowerpower_io.saver import MySQLWriter\nimport pandas as pd\n\ndf = pd.DataFrame({'id': [1, 2], 'customer': ['John', 'Jane']})\nmysql_writer = MySQLWriter(\n    database=\"mydb\",\n    user=\"myuser\",\n    password=\"mypassword\",\n    host=\"localhost\",\n    table_name=\"customers\"\n)\nmysql_writer.write(data=df)\n</code></pre></p>"},{"location":"api/saver/#mssqlwriter","title":"<code>MSSQLWriter</code>","text":"<p>Writes data to Microsoft SQL Server databases. - Path: <code>src/flowerpower_io/saver/mssql.py</code> - Example:   <pre><code>from flowerpower_io.saver import MSSQLWriter\nimport pandas as pd\n\ndf = pd.DataFrame({'id': [1, 2], 'order_id': [101, 102]})\nmssql_writer = MSSQLWriter(\n    database=\"mydb\",\n    user=\"myuser\",\n    password=\"mypassword\",\n    host=\"localhost\",\n    table_name=\"orders\"\n)\nmssql_writer.write(data=df)\n</code></pre></p>"},{"location":"api/saver/#oracledbwriter","title":"<code>OracleDBWriter</code>","text":"<p>Writes data to Oracle databases. - Path: <code>src/flowerpower_io/saver/oracle.py</code> - Example:   ```python   from flowerpower_io.saver import OracleDBWriter   import pandas as pd</p> <p>df = pd.DataFrame({'id': [1, 2], 'item': ['Pen', 'Book']})   oracle_writer = OracleDBWriter(       database=\"mydb\",       user=\"myuser\",       password=\"mypassword\",       host=\"localhost\",       table_name=\"items\"   )   oracle_writer.write(data=df)</p>"}]}